\section{Fundamentos de Aprendizaje Profundo para Visión por Computadora}

El aprendizaje profundo (\textit{deep learning}) ha revolucionado el campo de la visión por computadora en la última década, permitiendo el desarrollo de sistemas capaces de aprender representaciones jerárquicas de características directamente desde datos en bruto, sin la necesidad de ingeniería manual de características. En el contexto de la detección de \textit{landmarks} anatómicos, las Redes Neuronales Convolucionales (\textit{CNNs}) representan la arquitectura fundamental que sustenta los métodos del estado del arte. Esta sección establece los fundamentos matemáticos y computacionales necesarios para comprender las arquitecturas residuales (Sección~2.3), las estrategias de aprendizaje por transferencia (Sección~2.4), y las funciones de pérdida especializadas (Sección~2.5) que constituyen los componentes técnicos centrales de este trabajo \cite{Goodfellow2016deep, LeCun2015deep}.

\subsection{Redes Neuronales y Representaciones Jerárquicas}

Las redes neuronales artificiales profundas se componen de múltiples capas de transformaciones no lineales que procesan información de manera jerárquica. En el caso del perceptrón multicapa básico, cada neurona en la capa $l$ computa una combinación lineal de las activaciones de la capa anterior seguida de una función de activación no lineal:
\begin{equation}
a_j^{(l)} = f\left(\sum_{i=1}^{n} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)}\right)
\label{eq:perceptron}
\end{equation}
donde $w_{ij}^{(l)}$ representa el peso de conexión de la neurona $i$ en la capa $l-1$ a la neurona $j$ en la capa $l$, $b_j^{(l)}$ es el término de sesgo, y $f(\cdot)$ es la función de activación no lineal.

El concepto de representaciones jerárquicas es fundamental en \textit{deep learning}: las capas tempranas de la red aprenden a detectar características de bajo nivel (bordes, texturas, gradientes), mientras que las capas profundas componen estas características simples para formar representaciones de alto nivel (formas complejas, objetos completos, relaciones espaciales). Esta jerarquía de abstracciones es particularmente adecuada para el procesamiento de imágenes médicas, donde la detección de estructuras anatómicas complejas requiere la integración de información visual a múltiples escalas \cite{Goodfellow2016deep}.

Sin embargo, las arquitecturas basadas en capas completamente conectadas (\textit{fully connected}) presentan limitaciones severas para el procesamiento de imágenes. Una imagen de radiografía de tórax de dimensiones modestas ($256 \times 256$ píxeles con un canal de intensidad) contiene 65,536 valores de entrada. Una capa completamente conectada con 1,000 neuronas requeriría 65.5 millones de parámetros solo en la primera capa, resultando en un modelo computacionalmente intratable y altamente susceptible al sobreajuste. Esta limitación motivó el desarrollo de arquitecturas convolucionales que explotan la estructura espacial de las imágenes mediante compartición de parámetros y conectividad local \cite{LeCun1998gradient}.

\subsection{Redes Neuronales Convolucionales}

Las Redes Neuronales Convolucionales (\textit{Convolutional Neural Networks}, CNNs) constituyen una clase especializada de redes neuronales diseñadas para procesar datos con topología de rejilla, como imágenes bidimensionales. La operación fundamental de las CNNs es la convolución discreta, definida matemáticamente para imágenes bidimensionales como:
\begin{equation}
Y[i,j] = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} X[i+m, j+n] \cdot W[m,n] + b
\label{eq:convolution}
\end{equation}
donde $X$ representa la imagen de entrada con dimensiones $H \times W$, $W$ es el kernel o filtro convolucional de dimensiones $M \times N$ (típicamente $3 \times 3$ o $5 \times 5$), $Y$ es el mapa de características de salida (\textit{feature map}), y $b$ es el término de sesgo compartido por todas las ubicaciones espaciales.

La aplicación de la convolución está controlada por dos hiperparámetros adicionales: el paso (del inglés, \textit{stride}) $S$, que determina el desplazamiento del kernel entre aplicaciones consecutivas, y el relleno (del inglés, \textit{padding}) $P$, que especifica el número de píxeles añadidos en los bordes de la imagen de entrada. La dimensión espacial de la salida se calcula mediante:
\begin{equation}
H_{out} = \left\lfloor \frac{H_{in} + 2P - K}{S} \right\rfloor + 1, \quad
W_{out} = \left\lfloor \frac{W_{in} + 2P - K}{S} \right\rfloor + 1
\label{eq:output_dimensions}
\end{equation}
donde $K$ representa el tamaño del kernel (asumiendo kernels cuadrados $K \times K$). El \textit{padding} se utiliza frecuentemente para preservar las dimensiones espaciales ($P = \lfloor K/2 \rfloor$ con $S=1$ mantiene $H_{out} = H_{in}$), mientras que valores de \textit{stride} mayores a 1 reducen las dimensiones espaciales, proporcionando una forma de submuestreo.

El campo receptivo (del inglés, \textit{receptive field}) de una neurona en una capa profunda define la región de la imagen de entrada que influye en su activación. En CNNs, el campo receptivo crece exponencialmente con la profundidad de la red: una neurona en la capa $L$ con kernels de tamaño $K$ tiene un campo receptivo de tamaño aproximado $(K-1)L + 1$. Este crecimiento permite que capas profundas integren información de regiones cada vez más extensas de la imagen, capturando contexto espacial relevante para la tarea de detección.

Las capas convolucionales presentan tres propiedades arquitectónicas fundamentales que las hacen superiores a capas completamente conectadas para visión por computadora \cite{LeCun1998gradient, Krizhevsky2012}:

\begin{enumerate}
    \item \textbf{Compartición de parámetros:} El mismo filtro se aplica en todas las ubicaciones espaciales de la imagen, reduciendo drásticamente el número de parámetros. Un kernel de $3 \times 3$ con 64 filtros requiere solo $3 \times 3 \times 64 = 576$ parámetros (más 64 sesgos), independientemente del tamaño de la imagen de entrada.

    \item \textbf{Invarianza traslacional:} Características detectadas en una región de la imagen pueden ser detectadas en cualquier otra región mediante el mismo conjunto de pesos, proporcionando robustez a traslaciones del objeto de interés.

    \item \textbf{Conectividad local:} Cada neurona procesa solo una región local de la entrada, explotando la correlación espacial inherente en imágenes naturales y médicas.
\end{enumerate}

Una capa convolucional típica aplica múltiples filtros en paralelo, donde cada filtro aprende a detectar una característica específica (bordes horizontales, verticales, gradientes de intensidad, texturas). La salida de una capa convolucional es un tensor tridimensional de dimensiones $H_{out} \times W_{out} \times D_{out}$, donde $D_{out}$ representa el número de filtros aplicados. Las capas tempranas de CNNs profundas aprenden detectores de características de bajo nivel, mientras que capas subsecuentes componen estas características para formar representaciones jerárquicamente más abstractas \cite{Krizhevsky2012}.

\subsection{Operaciones de Submuestreo y Funciones de Activación}

Las operaciones de submuestreo (del inglés, \textit{pooling}) reducen progresivamente las dimensiones espaciales de las representaciones intermedias, disminuyendo la carga computacional y el número de parámetros, mientras expanden el campo receptivo efectivo de las capas subsecuentes. La operación de submuestreo máximo (\textit{max pooling}) es la más ampliamente utilizada en arquitecturas modernas, definida como:
\begin{equation}
Y[i,j] = \max_{m,n \in R_{ij}} X[m,n]
\label{eq:maxpooling}
\end{equation}
donde $R_{ij}$ representa la región de \textit{pooling}, típicamente de tamaño $2 \times 2$ con \textit{stride} de 2, lo que reduce las dimensiones espaciales a la mitad. Alternativamente, el submuestreo promedio (\textit{average pooling}) calcula la media aritmética de los valores en la región:
\begin{equation}
Y[i,j] = \frac{1}{|R_{ij}|} \sum_{m,n \in R_{ij}} X[m,n]
\label{eq:avgpooling}
\end{equation}

El \textit{max pooling} proporciona invarianza a pequeñas traslaciones y deformaciones locales, preservando la activación máxima (más fuerte) dentro de cada región. Esta propiedad es particularmente útil para tareas de detección donde la presencia de una característica es más relevante que su ubicación precisa dentro de una región local.

Las funciones de activación no lineales son componentes esenciales de las redes neuronales, ya que permiten a la red aprender transformaciones no lineales complejas. La función de activación más ampliamente utilizada en CNNs modernas es la Unidad Lineal Rectificada (\textit{Rectified Linear Unit}, ReLU), definida como:
\begin{equation}
f(x) = \max(0, x) = \begin{cases}
x & \text{si } x > 0 \\
0 & \text{si } x \leq 0
\end{cases}
\label{eq:relu}
\end{equation}

La derivada de ReLU es particularmente simple:
\begin{equation}
f'(x) = \begin{cases}
1 & \text{si } x > 0 \\
0 & \text{si } x \leq 0
\end{cases}
\label{eq:relu_derivative}
\end{equation}

ReLU presenta ventajas significativas sobre funciones de activación clásicas como sigmoide y tangente hiperbólica \cite{Krizhevsky2012}: (1) no exhibe saturación en la región positiva, evitando el problema del gradiente desvaneciente que afecta a redes profundas con activaciones sigmoideas; (2) su evaluación es computacionalmente eficiente, requiriendo solo una operación de comparación y selección máxima; (3) induce \textit{sparsity} en las representaciones, ya que aproximadamente 50\% de las activaciones son cero, lo que puede mejorar la eficiencia y la generalización.

Una limitación de ReLU es el fenómeno conocido como ``dying ReLU'', donde neuronas que consistentemente reciben entradas negativas producen activaciones de cero y dejan de aprender, ya que sus gradientes son nulos. Variantes como Leaky ReLU ($f(x) = \max(0.01x, x)$) y Parametric ReLU (PReLU) abordan parcialmente esta limitación al permitir gradientes pequeños para valores negativos.

Otras funciones de activación relevantes incluyen la sigmoide, $\sigma(x) = 1/(1+e^{-x})$, que comprime valores al rango $(0,1)$ pero sufre de gradientes desvanecientes para valores extremos; la tangente hiperbólica, $\tanh(x) = (e^x - e^{-x})/(e^x + e^{-x})$, que mapea al rango $(-1,1)$; y \textit{softmax}, utilizada en capas de salida para tareas de clasificación multi-clase:
\begin{equation}
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}
\label{eq:softmax}
\end{equation}
donde $K$ es el número de clases. La función \textit{softmax} garantiza que las salidas sean no negativas y sumen uno, interpretándose como probabilidades posteriores de clase.

\subsection{Algoritmo de Retropropagación}

El entrenamiento de redes neuronales profundas se realiza mediante el algoritmo de retropropagación (del inglés, \textit{backpropagation}), que calcula eficientemente el gradiente de una función de pérdida $\mathcal{L}$ respecto a todos los parámetros de la red mediante aplicación recursiva de la regla de la cadena del cálculo diferencial \cite{Rumelhart1986, Goodfellow2016deep}. Considérese una red neuronal con $L$ capas, donde cada capa $l$ realiza la transformación:
\begin{align}
z^{(l)} &= W^{(l)} a^{(l-1)} + b^{(l)} \label{eq:forward_linear} \\
a^{(l)} &= f(z^{(l)}) \label{eq:forward_activation}
\end{align}
donde $z^{(l)}$ representa la entrada ponderada (\textit{pre-activation}), $a^{(l)}$ es la activación de la capa $l$, $W^{(l)}$ y $b^{(l)}$ son los parámetros (pesos y sesgos), y $f(\cdot)$ es la función de activación. La propagación hacia adelante (\textit{forward pass}) evalúa estas ecuaciones secuencialmente desde la entrada hasta la salida.

El objetivo del entrenamiento es minimizar una función de pérdida $\mathcal{L}(a^{(L)}, y)$ que cuantifica la discrepancia entre la predicción de la red $a^{(L)}$ y el valor objetivo $y$. Para actualizar los parámetros mediante descenso de gradiente, se requiere calcular $\partial \mathcal{L}/\partial W^{(l)}$ y $\partial \mathcal{L}/\partial b^{(l)}$ para toda capa $l$. La retropropagación logra esto mediante la definición del error de retropropagación $\delta^{(l)}$ en cada capa:
\begin{equation}
\delta^{(l)} = \frac{\partial \mathcal{L}}{\partial z^{(l)}}
\label{eq:delta_definition}
\end{equation}

Para la capa de salida $L$, el error de retropropagación se calcula directamente mediante la regla de la cadena:
\begin{equation}
\delta^{(L)} = \frac{\partial \mathcal{L}}{\partial a^{(L)}} \odot f'(z^{(L)})
\label{eq:delta_output}
\end{equation}
donde $\odot$ denota el producto elemento a elemento (producto de Hadamard). Para capas intermedias, el error se propaga hacia atrás mediante:
\begin{equation}
\delta^{(l)} = \left((W^{(l+1)})^T \delta^{(l+1)}\right) \odot f'(z^{(l)})
\label{eq:delta_backprop}
\end{equation}

Esta ecuación recursiva constituye el núcleo del algoritmo de retropropagación: el error en la capa $l$ se obtiene multiplicando el error de la capa siguiente por la matriz de pesos transpuesta (propagación del error hacia atrás a través de la transformación lineal), seguido de una modulación elemento a elemento por la derivada de la función de activación.

Una vez calculados los errores $\delta^{(l)}$ para todas las capas, los gradientes respecto a los parámetros se obtienen como:
\begin{align}
\frac{\partial \mathcal{L}}{\partial W^{(l)}} &= \delta^{(l)} (a^{(l-1)})^T \label{eq:gradient_weights} \\
\frac{\partial \mathcal{L}}{\partial b^{(l)}} &= \delta^{(l)} \label{eq:gradient_bias}
\end{align}

Los parámetros se actualizan mediante descenso de gradiente:
\begin{equation}
W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial W^{(l)}}, \quad
b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial \mathcal{L}}{\partial b^{(l)}}
\label{eq:parameter_update}
\end{equation}
donde $\eta$ es la tasa de aprendizaje (\textit{learning rate}), un hiperparámetro que controla la magnitud de las actualizaciones de parámetros.

La complejidad computacional de la retropropagación es del mismo orden que la propagación hacia adelante, típicamente $O(W)$ donde $W$ es el número total de pesos en la red. Esta eficiencia computacional, combinada con la disponibilidad de unidades de procesamiento gráfico (GPUs) altamente paralelizables, ha permitido el entrenamiento de redes con cientos de millones de parámetros en conjuntos de datos masivos.

\subsection{Algoritmos de Optimización}

El algoritmo básico de descenso de gradiente estocástico (\textit{Stochastic Gradient Descent}, SGD) actualiza los parámetros $\theta$ de la red utilizando el gradiente calculado sobre una muestra individual o un mini-lote pequeño de datos:
\begin{equation}
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t; x^{(i)}, y^{(i)})
\label{eq:sgd}
\end{equation}
donde $t$ indexa la iteración de actualización, y $(x^{(i)}, y^{(i)})$ representa una muestra de entrenamiento. A diferencia del descenso de gradiente por lotes que utiliza el conjunto de entrenamiento completo, SGD proporciona actualizaciones frecuentes que aceleran la convergencia, aunque con mayor varianza en la dirección de descenso.

Una mejora fundamental sobre SGD es la incorporación de momentum, que acumula un promedio móvil exponencialmente ponderado de gradientes pasados:
\begin{align}
v_t &= \beta v_{t-1} + \eta \nabla_\theta \mathcal{L}(\theta_t) \label{eq:momentum_velocity} \\
\theta_{t+1} &= \theta_t - v_t \label{eq:momentum_update}
\end{align}
donde $v_t$ representa la velocidad acumulada, y $\beta$ es el coeficiente de momentum (típicamente 0.9). El momentum reduce oscilaciones en direcciones de alta curvatura y acelera la convergencia en direcciones consistentes del espacio de parámetros \cite{Goodfellow2016deep}.

El optimizador Adam (\textit{Adaptive Moment Estimation}) representa el estado del arte en algoritmos de optimización para \textit{deep learning}, combinando las ventajas de momentum con tasas de aprendizaje adaptativas por parámetro \cite{Kingma2014adam}. Adam mantiene estimaciones de los momentos de primer y segundo orden de los gradientes:
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) g_t \label{eq:adam_first_moment} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \label{eq:adam_second_moment}
\end{align}
donde $g_t = \nabla_\theta \mathcal{L}(\theta_t)$ es el gradiente en el tiempo $t$, $m_t$ es el primer momento (media), $v_t$ es el segundo momento no centrado (varianza no centrada), y $\beta_1, \beta_2 \in [0,1)$ son tasas de decaimiento exponencial (valores típicos: $\beta_1 = 0.9$, $\beta_2 = 0.999$).

Dado que $m_t$ y $v_t$ se inicializan en cero, presentan sesgo hacia cero en las primeras iteraciones. Adam corrige este sesgo mediante:
\begin{equation}
\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}
\label{eq:adam_bias_correction}
\end{equation}

La actualización de parámetros incorpora una tasa de aprendizaje adaptativa por parámetro:
\begin{equation}
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\label{eq:adam_update}
\end{equation}
donde $\epsilon = 10^{-8}$ es una constante pequeña para estabilidad numérica. El término $\sqrt{\hat{v}_t}$ normaliza el tamaño de actualización por parámetro basándose en la magnitud histórica de los gradientes, proporcionando actualizaciones más grandes para parámetros con gradientes consistentemente pequeños y actualizaciones más pequeñas para parámetros con gradientes grandes o ruidosos.

Adam ha demostrado convergencia robusta en una amplia variedad de arquitecturas de \textit{deep learning} y es particularmente efectivo en aplicaciones de visión médica, donde los conjuntos de datos suelen ser de tamaño moderado y la optimización cuidadosa es crítica para evitar sobreajuste \cite{Litjens2017}. La combinación de momentum adaptativo y tasas de aprendizaje por parámetro permite que Adam funcione razonablemente bien con hiperparámetros por defecto, reduciendo la necesidad de ajuste extenso de hiperparámetros.
