\section{Funciones de Pérdida para Regresión de Coordenadas}

La estrategia de aprendizaje por transferencia establecida en la Sección~2.4 proporciona una inicialización favorable de los pesos de la red mediante pre-entrenamiento en ImageNet, pero la función de pérdida determina fundamentalmente qué aprende la red durante el \textit{fine-tuning} en el dominio objetivo. Para la tarea de detección de \textit{landmarks} anatómicos, la red debe aprender un mapeo $f_\theta: \mathbb{R}^{H \times W} \to \mathbb{R}^{2K}$ desde una imagen de entrada de dimensiones $H \times W$ a un vector de $2K$ valores continuos representando las coordenadas $(x_k, y_k)$ de $K$ \textit{landmarks}. Esta tarea de regresión de coordenadas presenta desafíos específicos: requiere precisión a nivel de píxel individual, debe ser robusta ante variabilidad anatómica y calidad de imagen heterogénea, y puede beneficiarse de la incorporación explícita de conocimiento anatómico a priori mediante restricciones geométricas. Esta sección analiza funciones de pérdida especializadas para regresión de coordenadas, comenzando con el error cuadrático medio como línea base, seguido por \textit{Wing Loss} que amplifica gradientes para errores pequeños, y concluyendo con funciones de pérdida basadas en restricciones geométricas que incorporan conocimiento anatómico de simetría bilateral y preservación de distancias \cite{Feng2018, Noothout2020, Cheng2023}.

\subsection{Error Cuadrático Medio}

El Error Cuadrático Medio (del inglés, \textit{Mean Squared Error}, MSE) constituye la función de pérdida estándar para tareas de regresión, incluyendo regresión de coordenadas de \textit{landmarks}. Para un conjunto de $K$ \textit{landmarks}, donde cada \textit{landmark} $k$ tiene coordenadas ground truth $(x_k, y_k)$ y coordenadas predichas $(\hat{x}_k, \hat{y}_k)$, MSE se define como:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{K} \sum_{k=1}^{K} \left[\left(x_k - \hat{x}_k\right)^2 + \left(y_k - \hat{y}_k\right)^2\right]
\label{eq:mse_explicit}
\end{equation}

Esta formulación puede expresarse de manera más compacta utilizando notación vectorial. Definiendo $p_k = (x_k, y_k)^T \in \mathbb{R}^2$ como el vector de posición del \textit{landmark} $k$ y $\hat{p}_k = (\hat{x}_k, \hat{y}_k)^T$ como su predicción correspondiente, la función de pérdida MSE se escribe:
\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{K} \sum_{k=1}^{K} \|p_k - \hat{p}_k\|_2^2 = \frac{1}{K} \sum_{k=1}^{K} (p_k - \hat{p}_k)^T(p_k - \hat{p}_k)
\label{eq:mse_vector}
\end{equation}
donde $\|\cdot\|_2$ denota la norma Euclidiana. El gradiente de MSE respecto a la predicción del \textit{landmark} $k$ es:
\begin{equation}
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial \hat{p}_k} = \frac{2}{K}(\hat{p}_k - p_k)
\label{eq:mse_gradient}
\end{equation}

MSE posee propiedades matemáticas deseables: es una función convexa, diferenciable en todos los puntos, y su mínimo global coincide con la media de los datos objetivo. Durante la retropropagación, el gradiente proporcionado por MSE crece linealmente con la magnitud del error de predicción ($\|\nabla \mathcal{L}_{\text{MSE}}\| \propto \|p_k - \hat{p}_k\|$). Sin embargo, esta característica resulta problemática para la detección precisa de \textit{landmarks} anatómicos por múltiples razones \cite{Feng2018, Liu2021}.

Primero, MSE exhibe \textbf{sensibilidad desbalanceada a errores de diferente magnitud}. \textit{Landmarks} con predicciones muy incorrectas (errores de decenas de píxeles) generan gradientes dominantes que pueden enmascarar la señal de aprendizaje de \textit{landmarks} con errores pequeños (1-2 píxeles). Segundo, la penalización cuadrática amplifica el impacto de valores atípicos (\textit{outliers}): un solo \textit{landmark} mal predicho contribuye con $error^2$ a la pérdida total, potencialmente desestabilizando el entrenamiento en presencia de oclusiones parciales o artefactos de imagen. Tercero, para errores pequeños ($\|p_k - \hat{p}_k\| < 1$ píxel), el gradiente de MSE se vuelve proporcionalmente pequeño ($\|\nabla \mathcal{L}_{\text{MSE}}\| \approx 2|error|/K \ll 1$), debilitando la señal de aprendizaje precisamente en el régimen donde se requiere refinamiento fino de coordenadas. Finalmente, MSE trata cada \textit{landmark} independientemente, ignorando restricciones geométricas inherentes a la anatomía torácica como simetría bilateral y distancias anatómicas características.

Estas limitaciones motivaron el desarrollo de funciones de pérdida especializadas que amplifican gradientes en el régimen de errores pequeños mientras mantienen robustez ante errores grandes, y que incorporan conocimiento anatómico a priori mediante términos de regularización geométrica.

\subsection{Wing Loss: Amplificación de Gradientes para Errores Pequeños}

Feng et al.~\cite{Feng2018} propusieron \textit{Wing Loss} como una función de pérdida diseñada específicamente para localización robusta de \textit{landmarks} faciales, abordando las limitaciones de MSE mediante amplificación selectiva de gradientes en la región de errores pequeños. La idea central es modificar el comportamiento de la función de pérdida para proporcionar gradientes grandes cuando el error de predicción es pequeño (facilitando refinamiento preciso de coordenadas), mientras se mantienen gradientes moderados para errores grandes (proporcionando robustez).

\textit{Wing Loss} se define mediante una función no lineal por partes:
\begin{equation}
\mathcal{L}_{\text{wing}}(x) = \begin{cases}
w \ln\left(1 + \frac{|x|}{\epsilon}\right) & \text{si } |x| < w \\
|x| - C & \text{si } |x| \geq w
\end{cases}
\label{eq:wing_loss_definition}
\end{equation}
donde $x$ representa el error de coordenada, $w$ es el ancho de la región no lineal (típicamente $w \in [5, 10]$ píxeles para imágenes médicas), $\epsilon$ es un parámetro de curvatura que controla la suavidad de la transición (típicamente $\epsilon = 2.0$), y $C = w - w\ln(1 + w/\epsilon)$ es una constante que garantiza continuidad de la función en $|x| = w$. La región $|x| < w$ exhibe comportamiento logarítmico que amplifica gradientes para errores pequeños, mientras que la región $|x| \geq w$ presenta comportamiento lineal similar a la pérdida L1 absoluta, proporcionando robustez ante \textit{outliers}.

Para la detección de $K$ \textit{landmarks}, \textit{Wing Loss} se aplica al error radial de cada \textit{landmark}:
\begin{equation}
\mathcal{L}_{\text{Wing}} = \frac{1}{K} \sum_{k=1}^{K} \mathcal{L}_{\text{wing}}\left(\|p_k - \hat{p}_k\|_2\right)
\label{eq:wing_loss_landmarks}
\end{equation}

El comportamiento de amplificación de gradientes de \textit{Wing Loss} se comprende mediante el análisis de su derivada. Para $|x| < w$, la derivada es:
\begin{equation}
\frac{\partial \mathcal{L}_{\text{wing}}(x)}{\partial x} = \frac{w}{\epsilon + |x|} \cdot \text{sign}(x)
\label{eq:wing_gradient_small}
\end{equation}
donde $\text{sign}(x) = \pm 1$ indica la dirección del error. Para $|x| \geq w$, la derivada es:
\begin{equation}
\frac{\partial \mathcal{L}_{\text{wing}}(x)}{\partial x} = \text{sign}(x)
\label{eq:wing_gradient_large}
\end{equation}

La amplificación de gradientes se manifiesta en el límite de errores pequeños:
\begin{equation}
\lim_{x \to 0} \frac{\partial \mathcal{L}_{\text{wing}}(x)}{\partial x} = \frac{w}{\epsilon} \cdot \text{sign}(x)
\label{eq:wing_gradient_limit}
\end{equation}

Para la configuración típica $w = 5$ y $\epsilon = 2$, el gradiente en $x \to 0$ es $w/\epsilon = 2.5$, significativamente mayor que el gradiente de MSE en el mismo punto ($\partial \mathcal{L}_{\text{MSE}}/\partial x = 2x/K \approx 0$ cuando $x \to 0$). En el punto de transición $|x| = w$, el gradiente de \textit{Wing Loss} es exactamente $\pm 1$, garantizando continuidad de la derivada. Para errores grandes $|x| \gg w$, el gradiente satura en $\pm 1$, similar a la pérdida L1, proporcionando robustez ante predicciones extremadamente incorrectas que podrían desestabilizar el entrenamiento si se penalizaran cuadráticamente.

La comparación formal con MSE ilustra la ventaja de \textit{Wing Loss}. El gradiente de MSE respecto al error $x$ es $\partial \mathcal{L}_{\text{MSE}}/\partial x = 2x/K$, que decrece linealmente hacia cero a medida que el error disminuye. En contraste, \textit{Wing Loss} mantiene un gradiente constante y grande ($w/\epsilon$) en la región de errores pequeños, proporcionando una señal de aprendizaje consistente para el refinamiento fino de coordenadas. Esta propiedad es particularmente relevante para la detección de \textit{landmarks} anatómicos en radiografías de tórax, donde la variabilidad inter-paciente de posiciones de \textit{landmarks} es típicamente del orden de 10-20 píxeles, y se requiere precisión de localización a nivel de píxel individual para aplicaciones clínicas.

Extensiones recientes de \textit{Wing Loss} incluyen \textit{Adaptive Wing Loss} propuesto por Liu et al.~\cite{Liu2021}, que adapta dinámicamente los parámetros $w$ y $\epsilon$ durante el entrenamiento para equilibrar robustez inicial y precisión final. Cheng et al.~\cite{Cheng2023} demostraron en \textit{Medical Image Analysis} que la incorporación de perturbaciones controladas en las entradas combinada con \textit{Wing Loss} mejora significativamente la precisión de localización de \textit{landmarks} en imágenes médicas.

\subsection{Restricciones Geométricas: Symmetry Loss y Distance Preservation Loss}

Las funciones de pérdida basadas en regresión directa de coordenadas (MSE, \textit{Wing Loss}) tratan cada \textit{landmark} independientemente, ignorando relaciones geométricas inherentes a la anatomía humana. En el contexto específico de radiografías de tórax, la anatomía presenta propiedades geométricas consistentes que pueden explotarse como restricciones: la simetría bilateral aproximada del tórax implica que pares de \textit{landmarks} homólogos (izquierdo-derecho) deben ser aproximadamente simétricos respecto a la línea media, y las distancias entre \textit{landmarks} específicos exhiben variabilidad limitada en poblaciones sanas. La incorporación de estas restricciones geométricas como términos de regularización en la función de pérdida mejora la generalización del modelo y garantiza que las predicciones sean anatómicamente plausibles \cite{Urschler2021, Donner2013, Thaler2021}.

\subsubsection{Symmetry Loss}

Como se estableció en la Sección~2.1, los 15 \textit{landmarks} considerados en este trabajo incluyen siete pares de puntos con simetría bilateral respecto a la línea media vertical del tórax. Específicamente, los pares simétricos son: bordes costales laterales superiores (\#3, \#4), bordes costales laterales medios (\#5, \#6), bordes costales laterales inferiores (\#7, \#8), ápices pulmonares subclaviculares (\#12, \#13), y ángulos costofrénicos (\#14, \#15). Adicionalmente, el ángulo cardiofrénico izquierdo (\#2) debe ser aproximadamente simétrico respecto al eje definido por la escotadura yugular (\#1) y la carina traqueal (\#9).

La función de pérdida de simetría (\textit{Symmetry Loss}) penaliza desviaciones de esta simetría bilateral. Formalmente, sea $S = \{(3,4), (5,6), (7,8), (12,13), (14,15)\}$ el conjunto de pares de índices de \textit{landmarks} simétricos, y sea $p_c = (x_c, y_c)^T$ un punto de referencia en la línea media (que puede definirse como el promedio de las coordenadas $x$ de los \textit{landmarks} \#1 y \#9). La pérdida de simetría se define como:
\begin{equation}
\mathcal{L}_{\text{sym}} = \frac{1}{|S|} \sum_{(i,j) \in S} \left\|(p_i - p_c) + (p_j - p_c)\right\|_2^2
\label{eq:symmetry_loss}
\end{equation}

Esta formulación penaliza la suma vectorial $(p_i - p_c) + (p_j - p_c)$, que debería ser aproximadamente $(0, \Delta y)^T$ si los \textit{landmarks} $i$ y $j$ son perfectamente simétricos en la coordenada $x$ respecto a $p_c$, con posible diferencia en $y$ debido a asimetrías anatómicas menores. Una formulación alternativa más restrictiva penaliza exclusivamente desviaciones en la coordenada $x$:
\begin{equation}
\mathcal{L}_{\text{sym}}^{(x)} = \frac{1}{|S|} \sum_{(i,j) \in S} \left[(x_i - x_c) + (x_j - x_c)\right]^2
\label{eq:symmetry_loss_x}
\end{equation}

La pérdida de simetría proporciona regularización particularmente útil en presencia de oclusiones parciales o artefactos que afectan asimétricamente la imagen: si un \textit{landmark} en un hemitórax es difícil de detectar debido a oclusión, la restricción de simetría permite que el modelo infiera su posición aproximada basándose en la detección de su contraparte simétrica. Urschler et al.~\cite{Urschler2021} demostraron empíricamente que la incorporación de restricciones geométricas incluyendo simetría bilateral mejora consistentemente la precisión de detección de \textit{landmarks} en imágenes médicas, particularmente en conjuntos de datos pequeños donde la regularización es crítica.

\subsubsection{Distance Preservation Loss}

Las distancias Euclidianas entre pares específicos de \textit{landmarks} anatómicos exhiben variabilidad inter-paciente limitada en poblaciones normales, proporcionando una restricción geométrica adicional. Por ejemplo, la distancia entre los ápices pulmonares izquierdo y derecho (\#12, \#13) está relacionada con el ancho torácico superior, que varía dentro de un rango relativamente estrecho. La función de pérdida de preservación de distancias (\textit{Distance Preservation Loss}) penaliza predicciones que violan estas restricciones de distancia.

Formalmente, sea $D \subseteq \{1, \ldots, K\} \times \{1, \ldots, K\}$ un conjunto de pares de índices de \textit{landmarks} cuyas distancias deben preservarse, y sea $d_{ij}^{\text{ref}}$ la distancia de referencia entre \textit{landmarks} $i$ y $j$, típicamente estimada como la media de las distancias observadas en el conjunto de entrenamiento. La pérdida de preservación de distancias se define como:
\begin{equation}
\mathcal{L}_{\text{dist}} = \frac{1}{|D|} \sum_{(i,j) \in D} \left(\|p_i - p_j\|_2 - d_{ij}^{\text{ref}}\right)^2
\label{eq:distance_preservation}
\end{equation}

Esta formulación penaliza tanto la compresión excesiva (distancia predicha menor que $d_{ij}^{\text{ref}}$) como la expansión excesiva (distancia predicha mayor que $d_{ij}^{\text{ref}}$) de distancias anatómicas características. La selección del conjunto $D$ y las distancias de referencia $d_{ij}^{\text{ref}}$ constituye una decisión de diseño que requiere conocimiento anatómico: pares de \textit{landmarks} con alta correlación espacial y baja variabilidad inter-paciente son candidatos ideales. Thaler et al.~\cite{Thaler2021} propusieron métodos de análisis de forma basados en CT que identifican automáticamente restricciones de distancia anatómicamente significativas mediante análisis estadístico de formas.

Una limitación de \textit{Distance Preservation Loss} es que las distancias de referencia $d_{ij}^{\text{ref}}$ deben ser específicas de la población y potencialmente específicas de la condición patológica: pacientes con cardiomegalia exhibirán distancias características diferentes a pacientes con anatomía normal. No obstante, para restricciones suficientemente generales (como distancias entre estructuras óseas relativamente rígidas), esta función de pérdida proporciona regularización valiosa.

\subsubsection{Función de Pérdida Combinada}

En la práctica, las funciones de pérdida de regresión de coordenadas y las restricciones geométricas se combinan mediante suma ponderada:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{Wing}} + \lambda_2 \mathcal{L}_{\text{sym}} + \lambda_3 \mathcal{L}_{\text{dist}}
\label{eq:combined_loss}
\end{equation}
donde $\lambda_1, \lambda_2, \lambda_3 \geq 0$ son hiperparámetros que controlan el balance relativo entre precisión de coordenadas individuales y validez geométrica global. La elección de estos pesos constituye una decisión crítica: valores excesivos de $\lambda_2$ y $\lambda_3$ pueden forzar simetrías y distancias demasiado rígidas que no capturan la variabilidad anatómica real, mientras que valores demasiado pequeños no proporcionan suficiente regularización. Zeng et al.~\cite{Zeng2022} propusieron estrategias de aprendizaje auto-supervisado que aprenden automáticamente restricciones de consistencia geométrica desde datos no etiquetados, reduciendo la necesidad de especificación manual de restricciones y pesos.

La metodología específica de entrenamiento, incluyendo la selección de valores de hiperparámetros $\lambda_1, \lambda_2, \lambda_3$, las estrategias de ponderación adaptativa durante el entrenamiento, y los protocolos de validación experimental, se presentan en detalle en el Capítulo 3.
