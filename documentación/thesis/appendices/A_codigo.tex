% =============================================================================
% APENDICE A: FRAGMENTOS DE CODIGO RELEVANTES
% =============================================================================

\chapter{Fragmentos de Codigo Relevantes}
\label{ap:codigo}

Este apendice presenta los fragmentos de codigo mas importantes de la implementacion del sistema.

% -----------------------------------------------------------------------------
\section{Arquitectura del Modelo}
\label{sec:ap_arquitectura}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Clase principal del modelo ResNet18Landmarks},label={lst:ap_model}]
import torch
import torch.nn as nn
from torchvision import models

class ResNet18Landmarks(nn.Module):
    """
    Modelo de deteccion de landmarks basado en ResNet-18.

    Args:
        num_landmarks: Numero de landmarks a predecir (default: 15)
        pretrained: Usar pesos preentrenados de ImageNet
        coord_attention: Usar modulo Coordinate Attention
        deep_head: Usar cabeza de regresion profunda
        hidden_dim: Dimension de capa oculta (default: 768)
        dropout: Probabilidad de dropout (default: 0.3)
    """
    def __init__(
        self,
        num_landmarks=15,
        pretrained=True,
        coord_attention=True,
        deep_head=True,
        hidden_dim=768,
        dropout=0.3
    ):
        super().__init__()
        self.num_landmarks = num_landmarks

        # Backbone ResNet-18
        weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None
        resnet = models.resnet18(weights=weights)

        # Remover capa FC final
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])

        # Coordinate Attention (opcional)
        self.use_coord_attention = coord_attention
        if coord_attention:
            self.coord_attn = CoordinateAttention(512, reduction=32)

        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d(1)

        # Cabeza de regresion
        self.use_deep_head = deep_head
        if deep_head:
            self.head = nn.Sequential(
                nn.Linear(512, hidden_dim),
                nn.GroupNorm(32, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, 256),
                nn.GroupNorm(16, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(256, num_landmarks * 2),
                nn.Sigmoid()
            )
        else:
            self.head = nn.Sequential(
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
                nn.Linear(256, num_landmarks * 2),
                nn.Sigmoid()
            )

    def forward(self, x):
        # Backbone
        features = self.backbone(x)  # (B, 512, 7, 7)

        # Coordinate Attention
        if self.use_coord_attention:
            features = self.coord_attn(features)

        # Global Average Pooling
        features = self.gap(features)  # (B, 512, 1, 1)
        features = features.view(features.size(0), -1)  # (B, 512)

        # Cabeza de regresion
        output = self.head(features)  # (B, 30)

        return output

    def freeze_backbone(self):
        """Congela los pesos del backbone."""
        for param in self.backbone.parameters():
            param.requires_grad = False

    def unfreeze_backbone(self):
        """Descongela los pesos del backbone."""
        for param in self.backbone.parameters():
            param.requires_grad = True
\end{lstlisting}

% -----------------------------------------------------------------------------
\section{Coordinate Attention}
\label{sec:ap_coord_attn}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Implementacion del modulo Coordinate Attention},label={lst:ap_coord_attn}]
class CoordinateAttention(nn.Module):
    """
    Coordinate Attention Module (Hou et al., CVPR 2021).

    Captura dependencias espaciales de largo alcance mientras
    preserva informacion posicional precisa.
    """
    def __init__(self, in_channels, reduction=32):
        super().__init__()
        mip = max(8, in_channels // reduction)

        # Pooling direccional
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        # Convolucion 1x1 para codificacion conjunta
        self.conv1 = nn.Conv2d(in_channels, mip, kernel_size=1)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = nn.Hardswish()

        # Convolucion para mapas de atencion
        self.conv_h = nn.Conv2d(mip, in_channels, kernel_size=1)
        self.conv_w = nn.Conv2d(mip, in_channels, kernel_size=1)

    def forward(self, x):
        identity = x
        n, c, h, w = x.size()

        # Pooling horizontal y vertical
        x_h = self.pool_h(x)  # (n, c, h, 1)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # (n, c, w, 1)

        # Concatenar y codificar
        y = torch.cat([x_h, x_w], dim=2)  # (n, c, h+w, 1)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)

        # Separar y generar mapas de atencion
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)

        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()

        # Aplicar atencion
        return identity * a_w * a_h
\end{lstlisting}

% -----------------------------------------------------------------------------
\section{Wing Loss}
\label{sec:ap_wing_loss}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Implementacion de Wing Loss normalizada},label={lst:ap_wing_loss}]
import math

class WingLoss(nn.Module):
    """
    Wing Loss para deteccion de landmarks (Feng et al., CVPR 2018).

    Proporciona mayor sensibilidad a errores pequenos que MSE/L1.

    Args:
        omega: Umbral para cambio de regimen (default: 10)
        epsilon: Curvatura de la parte logaritmica (default: 2)
        normalized: Si True, escala parametros para coords en [0,1]
    """
    def __init__(self, omega=10, epsilon=2, normalized=True):
        super().__init__()
        scale = 224.0 if normalized else 1.0
        self.omega = omega / scale
        self.epsilon = epsilon / scale
        self.C = self.omega - self.omega * math.log(1 + self.omega / self.epsilon)

    def forward(self, pred, target):
        """
        Args:
            pred: Predicciones (B, 30) en [0, 1]
            target: Ground truth (B, 30) en [0, 1]

        Returns:
            Loss escalar
        """
        diff = torch.abs(pred - target)

        # Regimen logaritmico para errores pequenos
        # Regimen lineal para errores grandes
        loss = torch.where(
            diff < self.omega,
            self.omega * torch.log(1 + diff / self.epsilon),
            diff - self.C
        )

        return loss.mean()
\end{lstlisting}

% -----------------------------------------------------------------------------
\section{Dataset y Transformaciones}
\label{sec:ap_dataset}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Clase LandmarkDataset},label={lst:ap_dataset}]
import cv2
import numpy as np
from torch.utils.data import Dataset
from PIL import Image

class LandmarkDataset(Dataset):
    """
    Dataset para radiografias de torax con landmarks.

    Args:
        image_paths: Lista de rutas a imagenes
        landmarks: Array numpy de coordenadas (N, 15, 2)
        transform: Transformaciones a aplicar
        clahe: Aplicar CLAHE
        clahe_clip: Clip limit para CLAHE
        clahe_tile: Tile size para CLAHE
    """
    def __init__(
        self,
        image_paths,
        landmarks,
        transform=None,
        clahe=True,
        clahe_clip=2.0,
        clahe_tile=4
    ):
        self.image_paths = image_paths
        self.landmarks = landmarks
        self.transform = transform
        self.clahe = clahe
        self.clahe_clip = clahe_clip
        self.clahe_tile = clahe_tile

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Cargar imagen
        image = Image.open(self.image_paths[idx]).convert('RGB')
        image = np.array(image)

        # Aplicar CLAHE
        if self.clahe:
            image = self.apply_clahe(image)

        # Obtener landmarks y normalizar
        landmarks = self.landmarks[idx].copy()
        landmarks = landmarks / 299.0  # Normalizar a [0, 1]

        # Aplicar transformaciones
        if self.transform:
            image, landmarks = self.transform(image, landmarks)

        # Convertir a tensor
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        landmarks = torch.from_numpy(landmarks.flatten()).float()

        return image, landmarks

    def apply_clahe(self, image):
        """Aplica CLAHE en espacio de color LAB."""
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        clahe = cv2.createCLAHE(
            clipLimit=self.clahe_clip,
            tileGridSize=(self.clahe_tile, self.clahe_tile)
        )
        lab[:, :, 0] = clahe.apply(lab[:, :, 0])
        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
\end{lstlisting}

% -----------------------------------------------------------------------------
\section{Test-Time Augmentation}
\label{sec:ap_tta}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Implementacion de TTA para landmarks},label={lst:ap_tta}]
SYMMETRIC_PAIRS = [(2, 3), (4, 5), (6, 7), (11, 12), (13, 14)]

def predict_with_tta(model, image):
    """
    Prediccion con Test-Time Augmentation.

    Promedia predicciones de imagen original y flip horizontal.

    Args:
        model: Modelo entrenado
        image: Tensor de imagen (1, 3, 224, 224)

    Returns:
        Landmarks predichos (15, 2)
    """
    model.eval()
    predictions = []

    with torch.no_grad():
        # Prediccion original
        pred = model(image)
        pred = pred.view(-1, 15, 2)
        predictions.append(pred)

        # Prediccion con flip
        image_flip = torch.flip(image, dims=[3])
        pred_flip = model(image_flip)
        pred_flip = pred_flip.view(-1, 15, 2)

        # Invertir coordenada X
        pred_flip[:, :, 0] = 1.0 - pred_flip[:, :, 0]

        # Intercambiar pares simetricos
        for left, right in SYMMETRIC_PAIRS:
            pred_flip[:, [left, right]] = pred_flip[:, [right, left]]

        predictions.append(pred_flip)

    # Promediar
    final_pred = torch.stack(predictions).mean(dim=0)

    return final_pred.squeeze(0)
\end{lstlisting}

% -----------------------------------------------------------------------------
\section{Inferencia con Ensemble}
\label{sec:ap_ensemble}
% -----------------------------------------------------------------------------

\begin{lstlisting}[caption={Clase EnsemblePredictor},label={lst:ap_ensemble}]
class EnsemblePredictor:
    """
    Predictor que combina multiples modelos con TTA.

    Args:
        model_paths: Lista de rutas a checkpoints
        device: Dispositivo de computo (cuda/cpu)
    """
    def __init__(self, model_paths, device='cuda'):
        self.device = device
        self.models = []

        for path in model_paths:
            model = ResNet18Landmarks(
                coord_attention=True,
                deep_head=True,
                hidden_dim=768
            )
            checkpoint = torch.load(path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()
            self.models.append(model)

    def predict(self, image):
        """
        Predice landmarks para una imagen.

        Args:
            image: Imagen PIL o ruta a archivo

        Returns:
            Array numpy (15, 2) con coordenadas en pixeles
        """
        # Preprocesar
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        tensor = self.preprocess(image).to(self.device)

        # Predecir con cada modelo y TTA
        predictions = []
        for model in self.models:
            pred = predict_with_tta(model, tensor)
            predictions.append(pred)

        # Promediar ensemble
        ensemble_pred = torch.stack(predictions).mean(dim=0)

        # Convertir a pixeles
        landmarks = ensemble_pred.cpu().numpy() * 224.0

        return landmarks
\end{lstlisting}

% -----------------------------------------------------------------------------
% FIN DEL APENDICE
% -----------------------------------------------------------------------------
