% ==============================================================================
% DOCUMENTACIÓN CIENTÍFICA - ESTRATEGIA DE ENTRENAMIENTO EN DOS FASES
% Proyecto: Detección de COVID-19 mediante Landmarks Anatómicos
% Sesiones cubiertas: 3-4
% ==============================================================================

\documentclass[12pt,a4paper]{article}
\input{00_preambulo}

\title{Estrategia de Entrenamiento en Dos Fases\\para Detección de Landmarks Anatómicos}
\author{Documentación del Proceso de Desarrollo}
\date{Sesiones: 3-4}

\begin{document}
\maketitle

\begin{abstract}
Este documento presenta la estrategia de entrenamiento en dos fases desarrollada
para el modelo de predicción de landmarks anatómicos. La Fase 1 entrena únicamente
la cabeza de regresión con el backbone congelado (15 épocas), mientras que la
Fase 2 realiza fine-tuning completo con learning rates diferenciados para backbone
y cabeza. Esta estrategia, combinada con Wing Loss, redujo el error de predicción
de 26.65 píxeles (inicialización) a 9.08 píxeles en el conjunto de test, estableciendo
una base sólida para optimizaciones posteriores que alcanzarían el objetivo de $<$8 píxeles. Se documentan además
los bugs de escala encontrados en las funciones de pérdida auxiliares y su
corrección.
\end{abstract}

\tableofcontents
\newpage

% ==============================================================================
\section{Introducción y Motivación}
% ==============================================================================

El entrenamiento de redes neuronales profundas con transfer learning desde
ImageNet presenta un dilema: los pesos pre-entrenados del backbone son valiosos
para extracción de características, pero la cabeza de regresión debe inicializarse
aleatoriamente. Si se descongelan todos los parámetros desde el inicio, los
gradientes grandes de la cabeza mal inicializada pueden destruir las
representaciones útiles del backbone.

\begin{definicion}[Entrenamiento en Dos Fases]
Estrategia de transfer learning donde:
\begin{enumerate}
    \item \textbf{Fase 1}: El backbone permanece congelado mientras se entrena
    la cabeza con learning rate alto.
    \item \textbf{Fase 2}: Se descongela el backbone para fine-tuning con
    learning rate bajo, mientras la cabeza mantiene un learning rate moderado.
\end{enumerate}
\end{definicion}

\subsection{Justificación Teórica}

La transferencia de conocimiento desde ImageNet a radiografías de tórax no es
directa: las imágenes naturales tienen estadísticas de color y textura muy
diferentes a las radiografías médicas. Sin embargo, las capas iniciales del
backbone extraen características de bajo nivel (bordes, gradientes) que son
transferibles.

\begin{proposicion}[Transferibilidad de Capas]
En una red convolucional profunda, la transferibilidad disminuye con la
profundidad de la capa. Las capas iniciales (1-3) son altamente transferibles,
mientras que las capas finales son específicas del dominio original.
\end{proposicion}

Esto justifica usar learning rates diferenciados: las capas iniciales (más
transferibles) requieren ajustes mínimos, mientras que las capas finales
necesitan mayor adaptación.

% ==============================================================================
\section{Arquitectura del Pipeline de Entrenamiento}
% ==============================================================================

\subsection{Componentes del Sistema}

El pipeline de entrenamiento integra los siguientes componentes:

\begin{itemize}
    \item \textbf{LandmarkTrainer}: Clase principal que orquesta el entrenamiento
    \item \textbf{EarlyStopping}: Callback para detener entrenamiento si no hay mejora
    \item \textbf{ModelCheckpoint}: Guarda el mejor modelo según métrica objetivo
    \item \textbf{LRSchedulerCallback}: Gestiona el scheduler de learning rate
\end{itemize}

\subsection{Control de Congelamiento}

El modelo implementa métodos para controlar qué parámetros son entrenables:

\begin{lstlisting}[language=Python, caption={Control de congelamiento de parámetros}]
def freeze_all_except_head(self):
    """Congela backbone y coord_attention, deja head entrenable."""
    for param in self.features.parameters():
        param.requires_grad = False
    for param in self.coord_attention.parameters():
        param.requires_grad = False
    for param in self.head.parameters():
        param.requires_grad = True

def unfreeze_all(self):
    """Descongela todos los parametros."""
    for param in self.parameters():
        param.requires_grad = True
\end{lstlisting}

\subsection{Parámetros Entrenables por Fase}

\begin{table}[htbp]
\centering
\caption{Distribución de parámetros por fase de entrenamiento}
\label{tab:params_per_phase}
\begin{tabular}{lccc}
\toprule
\textbf{Componente} & \textbf{Parámetros} & \textbf{Fase 1} & \textbf{Fase 2} \\
\midrule
ResNet-18 Backbone & 11,176,512 & Congelado & Entrenable \\
Coordinate Attention & $\sim$2,000 & Congelado & Entrenable \\
Cabeza de Regresión & 137,038 & Entrenable & Entrenable \\
\midrule
\textbf{Total} & 11,315,550 & 137,038 & 11,315,550 \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Fase 1: Entrenamiento de Cabeza}
% ==============================================================================

\subsection{Configuración}

\begin{table}[htbp]
\centering
\caption{Hiperparámetros Fase 1}
\label{tab:phase1_hyperparams}
\begin{tabular}{lll}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor} & \textbf{Justificación} \\
\midrule
Épocas & 15 & Suficiente para convergencia de cabeza \\
Learning Rate & $1 \times 10^{-3}$ & Alto para entrenamiento rápido \\
Optimizador & Adam & Adaptativo, buena convergencia \\
Early Stopping Patience & 5 & Detener si no hay mejora \\
Parámetros Entrenables & Solo cabeza & Proteger backbone pre-entrenado \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Proceso de Entrenamiento}

\begin{algorithm}[H]
\caption{Fase 1: Entrenamiento de Cabeza}
\label{alg:phase1}
\begin{algorithmic}[1]
\REQUIRE DataLoaders $\mathcal{D}_{train}$, $\mathcal{D}_{val}$, modelo $M$, criterio $\mathcal{L}$
\ENSURE Modelo con cabeza entrenada
\STATE \COMMENT{Congelar backbone}
\STATE $M$.freeze\_all\_except\_head()
\STATE $\theta_{head} \leftarrow$ parámetros de $M$.head
\STATE optimizer $\leftarrow$ Adam($\theta_{head}$, lr=$10^{-3}$)
\STATE best\_error $\leftarrow \infty$
\STATE patience\_counter $\leftarrow 0$
\FOR{epoch $= 1$ \TO 15}
    \STATE \COMMENT{Training epoch}
    \FORALL{batch $(x, y)$ en $\mathcal{D}_{train}$}
        \STATE $\hat{y} \leftarrow M(x)$
        \STATE loss $\leftarrow \mathcal{L}(\hat{y}, y)$
        \STATE loss.backward()
        \STATE optimizer.step()
    \ENDFOR
    \STATE \COMMENT{Validation}
    \STATE val\_error $\leftarrow$ validate($M$, $\mathcal{D}_{val}$)
    \STATE \COMMENT{Early stopping check}
    \IF{val\_error $<$ best\_error}
        \STATE best\_error $\leftarrow$ val\_error
        \STATE save\_checkpoint($M$)
        \STATE patience\_counter $\leftarrow 0$
    \ELSE
        \STATE patience\_counter $\leftarrow$ patience\_counter $+ 1$
        \IF{patience\_counter $\geq 5$}
            \STATE \textbf{break}
        \ENDIF
    \ENDIF
\ENDFOR
\STATE load\_best\_checkpoint($M$)
\RETURN $M$
\end{algorithmic}
\end{algorithm}

\subsection{Resultados Fase 1}

\begin{table}[htbp]
\centering
\caption{Progreso de error durante Fase 1}
\label{tab:phase1_results}
\begin{tabular}{lccc}
\toprule
\textbf{Época} & \textbf{Train Error (px)} & \textbf{Val Error (px)} & \textbf{Observación} \\
\midrule
1 & 26.65 & 18.75 & Inicialización aleatoria \\
5 & 19.11 & 17.89 & Convergencia rápida \\
12 & 16.90 & 16.51 & \textbf{Mejor modelo} \\
15 & 16.44 & 16.78 & Fin de Phase 1 \\
\bottomrule
\end{tabular}
\end{table}

\begin{observacion}
La Fase 1 muestra convergencia estable durante las 15 épocas, donde solo 137K
parámetros de la cabeza son entrenables mientras el backbone de 11M parámetros
proporciona características estables. El mejor error de validación se alcanza
en la época 12 con 16.51 px.
\end{observacion}

% ==============================================================================
\section{Fase 2: Fine-tuning Completo}
% ==============================================================================

\subsection{Learning Rates Diferenciados}

La estrategia de learning rates diferenciados asigna tasas de aprendizaje
diferentes a cada grupo de parámetros:

\begin{equation}
\theta_t = \theta_{t-1} - \eta_i \cdot \nabla_{\theta} \mathcal{L}
\label{eq:lr_update}
\end{equation}

donde $\eta_i$ es el learning rate específico para el grupo de parámetros $i$:

\begin{equation}
\eta_i = \begin{cases}
2 \times 10^{-5} & \text{si } i \in \{\text{backbone}, \text{coord\_attention}\} \\
2 \times 10^{-4} & \text{si } i \in \{\text{head}\}
\end{cases}
\label{eq:lr_groups}
\end{equation}

\subsection{Justificación del Ratio 1:10}

El ratio de learning rates (backbone:head = 1:10) se justifica por:

\begin{enumerate}
    \item \textbf{Preservación de características}: El backbone contiene
    representaciones pre-entrenadas en ImageNet que son parcialmente
    transferibles. Un LR muy alto podría destruir estas representaciones.

    \item \textbf{Capacidad de la cabeza}: Con solo 137K parámetros vs 11M
    del backbone, la cabeza necesita LR mayor para tener gradientes
    proporcionalmente significativos.

    \item \textbf{Convergencia estable}: Empíricamente, ratios de 1:10 a 1:100
    proporcionan convergencia estable sin oscilaciones.
\end{enumerate}

\subsection{Scheduler Cosine Annealing}

Se utiliza Cosine Annealing para reducir gradualmente el learning rate:

\begin{equation}
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})\left(1 + \cos\left(\frac{t \cdot \pi}{T}\right)\right)
\label{eq:cosine_annealing}
\end{equation}

donde:
\begin{itemize}
    \item $\eta_{max}$: Learning rate inicial (diferente por grupo)
    \item $\eta_{min} = 10^{-6}$: Learning rate mínimo
    \item $T$: Número total de épocas (50)
    \item $t$: Época actual
\end{itemize}

\subsection{Configuración Fase 2}

\begin{table}[htbp]
\centering
\caption{Hiperparámetros Fase 2}
\label{tab:phase2_hyperparams}
\begin{tabular}{lll}
\toprule
\textbf{Hiperparámetro} & \textbf{Valor} & \textbf{Justificación} \\
\midrule
Épocas & 50 & Fine-tuning extenso \\
Backbone LR & $2 \times 10^{-5}$ & Ajuste fino del backbone \\
Head LR & $2 \times 10^{-4}$ & 10× mayor que backbone \\
Optimizador & Adam & Consistente con Fase 1 \\
Scheduler & CosineAnnealingLR & Reducción suave de LR \\
Early Stopping Patience & 10 & Mayor paciencia para fine-tuning \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Proceso de Entrenamiento Fase 2}

\begin{lstlisting}[language=Python, caption={Configuración de optimizador con LR diferenciado}]
# Descongelar todos los parametros
model.unfreeze_all()

# Grupos de parametros con LR diferenciado
param_groups = [
    {'params': model.features.parameters(),
     'lr': 2e-5},  # backbone
    {'params': model.coord_attention.parameters(),
     'lr': 2e-5},  # attention
    {'params': model.head.parameters(),
     'lr': 2e-4}   # cabeza (10x mayor)
]

optimizer = torch.optim.Adam(param_groups)

# Scheduler
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=50,      # epocas totales
    eta_min=1e-6   # LR minimo
)
\end{lstlisting}

\subsection{Resultados Fase 2}

\begin{table}[htbp]
\centering
\caption{Progreso de error durante Fase 2}
\label{tab:phase2_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Época} & \textbf{Train (px)} & \textbf{Val (px)} & \textbf{LR Backbone} & \textbf{Nota} \\
\midrule
1 & 15.61 & 15.36 & $2.0 \times 10^{-5}$ & Inicio \\
10 & 12.02 & 12.22 & $1.85 \times 10^{-5}$ & Mejora rápida \\
25 & 10.37 & 10.58 & $1.0 \times 10^{-5}$ & Medio ciclo \\
45 & 8.67 & 9.03 & $1.5 \times 10^{-6}$ & \textbf{Mejor modelo} \\
50 & 8.58 & 9.00 & $1.0 \times 10^{-6}$ & Fin entrenamiento \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Evaluación en Conjunto de Test}
% ==============================================================================

\subsection{Resultados Globales}

\begin{table}[htbp]
\centering
\caption{Estadísticas de error en test set}
\label{tab:test_stats}
\begin{tabular}{lc}
\toprule
\textbf{Métrica} & \textbf{Valor} \\
\midrule
Error medio & 9.08 px \\
Desviación estándar & 6.92 px \\
Error mediano & 7.28 px \\
Percentil 90 & 17.64 px \\
Percentil 95 & 21.20 px \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Resultados por Landmark}

\begin{table}[htbp]
\centering
\caption{Error por landmark en test set (ordenados)}
\label{tab:error_per_landmark}
\begin{tabular}{lccc}
\toprule
\textbf{Landmark} & \textbf{Error (px)} & \textbf{Tipo} & \textbf{Observación} \\
\midrule
L9 & 6.83 & Central & \textbf{Mejor} \\
L1 & 7.41 & Eje & Superior \\
L10 & 7.43 & Central & \\
L4 & 7.71 & Bilateral & Ápice \\
L5 & 8.38 & Bilateral & Hilio \\
L6 & 8.38 & Bilateral & Hilio \\
L3 & 8.44 & Bilateral & Ápice \\
L11 & 8.88 & Central & \\
L13 & 8.89 & Esquina & \\
L7 & 9.45 & Bilateral & Base \\
L12 & 9.59 & Esquina & Borde sup. \\
L8 & 10.19 & Bilateral & Base \\
L2 & 10.98 & Eje & Inferior \\
L14 & 11.68 & Bilateral & Costofrénico \\
L15 & 11.96 & Bilateral & \textbf{Peor} \\
\bottomrule
\end{tabular}
\end{table}

\begin{hallazgo}[title={Dificultad por ubicación anatómica}]
Los landmarks centrales (L9, L10, L11 sobre el eje L1-L2) son los más fáciles
de predecir ($\sim$6.8-8.9 px), mientras que los landmarks costofrénicos (L14, L15)
son los más difíciles ($\sim$11.7-12.0 px). Esto se debe a la menor
definición anatómica de la unión costoclavicular comparada con el eje central.
\end{hallazgo}

\subsection{Resultados por Categoría}

\begin{table}[htbp]
\centering
\caption{Error por categoría de diagnóstico}
\label{tab:error_per_category}
\begin{tabular}{lccc}
\toprule
\textbf{Categoría} & \textbf{Error (px)} & \textbf{N muestras} & \textbf{vs Normal} \\
\midrule
Normal & 3.42 & 47 & baseline \\
COVID-19 & 3.77 & 31 & +10.2\% \\
Viral Pneumonia & 4.40 & 18 & +28.7\% \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Comparación de Funciones de Pérdida}
% ==============================================================================

\subsection{Experimento: Wing Loss vs Combined Loss}

Se comparó Wing Loss simple contra una combinación de Wing Loss con
restricciones geométricas (Central Alignment + Soft Symmetry):

\begin{equation}
\mathcal{L}_{combined} = \mathcal{L}_{wing} + \lambda_1 \mathcal{L}_{central} + \lambda_2 \mathcal{L}_{symmetry}
\label{eq:combined_loss}
\end{equation}

\subsection{Bugs de Escala Encontrados}

\begin{observacion}[Bugs críticos en funciones de pérdida]
Durante la experimentación se descubrieron bugs de escala en las funciones
de pérdida auxiliares:
\begin{enumerate}
    \item \textbf{Wing Loss}: Parámetros $\omega=10$, $\epsilon=2$ estaban
    calibrados para píxeles (0-224), pero las coordenadas están normalizadas
    en $[0, 1]$.

    \item \textbf{CentralAlignmentLoss}: Multiplicaba por $224$ innecesariamente,
    generando valores 20× mayores que Wing Loss.

    \item \textbf{SoftSymmetryLoss}: Multiplicaba por $224^2 = 50176$, dominando
    completamente los gradientes.
\end{enumerate}
\end{observacion}

\subsection{Correcciones Implementadas}

\begin{lstlisting}[language=Python, caption={Corrección de escala en Wing Loss}]
class WingLoss(nn.Module):
    def __init__(self, omega=10, epsilon=2, normalized=True,
                 image_size=224):
        super().__init__()
        if normalized:
            # Escalar para coordenadas en [0, 1]
            self.omega = omega / image_size
            self.epsilon = epsilon / image_size
        else:
            self.omega = omega
            self.epsilon = epsilon
\end{lstlisting}

\subsection{Resultados Post-Corrección}

\begin{table}[htbp]
\centering
\caption{Comparación de funciones de pérdida (corregidas)}
\label{tab:loss_comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Función de Pérdida} & \textbf{Error Test (px)} & \textbf{Resultado} \\
\midrule
\textbf{Wing Loss} & \textbf{9.08} & Mejor \\
Combined Loss (corregida) & 10.52 & Funcional \\
\bottomrule
\end{tabular}
\end{table}

\begin{resultadoimportante}[title={Wing Loss es óptima para landmarks}]
Wing Loss simple supera a la combinación con restricciones geométricas por
0.57 píxeles. Las restricciones geométricas no aportan mejora porque:
\begin{enumerate}
    \item El ground truth ya satisface aproximadamente las restricciones
    \item Los gradientes adicionales pueden interferir con la convergencia
    \item Wing Loss ya proporciona gradientes útiles cerca del óptimo
\end{enumerate}
\end{resultadoimportante}

% ==============================================================================
\section{Análisis de Convergencia}
% ==============================================================================

\subsection{Curvas de Aprendizaje}

La convergencia del entrenamiento en dos fases sigue un patrón característico:

\begin{enumerate}
    \item \textbf{Fase 1 (épocas 1-7)}: Descenso rápido exponencial del error
    desde $\sim$80 px hasta $\sim$17 px.

    \item \textbf{Transición (época 7-8)}: Leve incremento al descongelar
    backbone debido a la perturbación de representaciones.

    \item \textbf{Fase 2 (épocas 8-55)}: Descenso más lento pero sostenido
    hasta $\sim$9 px, con oscilaciones menores.
\end{enumerate}

\subsection{Gap Train-Validation}

\begin{table}[htbp]
\centering
\caption{Gap de generalización por fase}
\label{tab:generalization_gap}
\begin{tabular}{lccc}
\toprule
\textbf{Fase} & \textbf{Train Final (px)} & \textbf{Val Final (px)} & \textbf{Gap} \\
\midrule
Fase 1 & 16.44 & 16.78 & -0.34 px \\
Fase 2 & 8.58 & 9.00 & -0.42 px \\
\bottomrule
\end{tabular}
\end{table}

El gap negativo indica que el modelo generaliza bien sin overfitting,
gracias a:
\begin{itemize}
    \item Early stopping con patience=10
    \item Dropout de 0.3 en la cabeza
    \item CosineAnnealingLR que reduce gradualmente el LR
\end{itemize}

% ==============================================================================
\section{Implementación del Trainer}
% ==============================================================================

\subsection{Clase LandmarkTrainer}

\begin{lstlisting}[language=Python, caption={Estructura del LandmarkTrainer}]
class LandmarkTrainer:
    """
    Trainer para entrenamiento en dos fases:
    - Phase 1: Backbone congelado, entrenar solo cabeza
    - Phase 2: Fine-tuning completo con LR diferenciado
    """

    def train_phase1(self, train_loader, val_loader, criterion,
                     epochs=15, lr=1e-3, patience=5):
        """Phase 1: Entrenar solo cabeza."""
        self.model.freeze_all_except_head()
        optimizer = Adam(self.model.head.parameters(), lr=lr)
        # ... training loop

    def train_phase2(self, train_loader, val_loader, criterion,
                     epochs=50, backbone_lr=2e-5, head_lr=2e-4,
                     patience=10):
        """Phase 2: Fine-tuning con LR diferenciado."""
        self.model.unfreeze_all()
        param_groups = self.model.get_trainable_params()
        param_groups[0]['lr'] = backbone_lr  # backbone
        param_groups[1]['lr'] = head_lr      # head
        optimizer = Adam(param_groups)
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
        # ... training loop

    def train_full(self, ...):
        """Entrenamiento completo en dos fases."""
        history1 = self.train_phase1(...)
        history2 = self.train_phase2(...)
        return {'phase1': history1, 'phase2': history2}
\end{lstlisting}

\subsection{Callbacks Implementados}

\begin{table}[htbp]
\centering
\caption{Callbacks del sistema de entrenamiento}
\label{tab:callbacks}
\begin{tabular}{lll}
\toprule
\textbf{Callback} & \textbf{Función} & \textbf{Parámetros} \\
\midrule
EarlyStopping & Detiene si no hay mejora & patience, mode='min' \\
ModelCheckpoint & Guarda mejor modelo & monitor='val\_error\_px' \\
LRSchedulerCallback & Gestiona scheduler & step\_on='epoch' \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Figuras Sugeridas}
% ==============================================================================

\subsection{Figura 5.1: Diagrama de Entrenamiento en Dos Fases}
\textit{Descripción}: Diagrama de flujo mostrando:
\begin{itemize}
    \item Fase 1: backbone congelado (gris), cabeza entrenable (verde)
    \item Transición: descongelamiento
    \item Fase 2: todo entrenable con LR diferenciados (backbone azul claro, cabeza verde oscuro)
\end{itemize}

\subsection{Figura 5.2: Curvas de Aprendizaje}
\textit{Descripción}: Gráfico con dos ejes:
\begin{itemize}
    \item Eje Y1: Error en píxeles (train y val)
    \item Eje Y2: Learning rate
    \item Eje X: Épocas (0-65 total)
    \item Línea vertical separando Fase 1 y Fase 2
\end{itemize}

\subsection{Figura 5.3: Error por Landmark}
\textit{Descripción}: Gráfico de barras horizontales mostrando error por
landmark (L1-L15), coloreado por tipo:
\begin{itemize}
    \item Azul: Landmarks centrales (L9, L10, L11)
    \item Verde: Landmarks del eje (L1, L2)
    \item Naranja: Landmarks bilaterales (resto)
\end{itemize}

\subsection{Figura 5.4: Error por Categoría}
\textit{Descripción}: Boxplot comparando distribución de errores por categoría
(Normal, Viral, COVID), mostrando que COVID tiene mayor mediana y varianza.

\subsection{Figura 5.5: Cosine Annealing LR}
\textit{Descripción}: Gráfico mostrando la curva de learning rate durante
Fase 2 para ambos grupos (backbone y head), desde LR inicial hasta $10^{-6}$.

% ==============================================================================
\section{Archivos Fuente}
% ==============================================================================

\begin{table}[htbp]
\centering
\caption{Archivos de implementación relevantes}
\label{tab:source_files}
\begin{tabular}{ll}
\toprule
\textbf{Archivo} & \textbf{Contenido} \\
\midrule
\archivo{src\_v2/training/trainer.py} & Clase LandmarkTrainer \\
\archivo{src\_v2/training/callbacks.py} & EarlyStopping, ModelCheckpoint \\
\archivo{src\_v2/models/losses.py} & Wing Loss corregida \\
\archivo{scripts/train.py} & Script principal de entrenamiento \\
\archivo{src\_v2/evaluation/metrics.py} & Métricas de evaluación \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Conclusiones}
% ==============================================================================

\begin{enumerate}
    \item \textbf{Estrategia de dos fases es efectiva}: Reduce el error de
    26.65 px (inicialización) a 9.08 px (test), estableciendo una base
    sólida que optimizaciones posteriores reducirían por debajo del objetivo de $<$8 px.

    \item \textbf{LR diferenciados son cruciales}: El ratio 1:10 entre backbone
    y cabeza permite fine-tuning estable sin destruir representaciones
    pre-entrenadas.

    \item \textbf{Wing Loss supera a Combined Loss}: Las restricciones
    geométricas auxiliares no aportan mejora; Wing Loss simple es suficiente.

    \item \textbf{Early stopping previene overfitting}: Con patience de 5-10
    épocas, el entrenamiento se detiene antes de sobreajustar.

    \item \textbf{Cosine Annealing suaviza convergencia}: La reducción gradual
    del LR evita oscilaciones al final del entrenamiento.

    \item \textbf{Viral Pneumonia es la categoría más difícil}: Error de 4.40 px vs
    3.42 px en Normal, aunque la diferencia es menor de lo esperado gracias
    a la optimización del modelo.

    \item \textbf{Bugs de escala son críticos}: Las funciones de pérdida
    deben operar en la misma escala que las coordenadas (normalizadas).
\end{enumerate}

\end{document}
