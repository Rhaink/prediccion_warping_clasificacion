% ==============================================================================
% DOCUMENTACIÓN CIENTÍFICA - OPTIMIZACIÓN DE ARQUITECTURA
% Proyecto: Detección de COVID-19 mediante Landmarks Anatómicos
% Sesiones cubiertas: 5, 6, 9
% ==============================================================================

\documentclass[12pt,a4paper]{article}
\input{00_preambulo}

\title{Optimización de Arquitectura para\\Detección de Landmarks Anatómicos}
\author{Documentación del Proceso de Desarrollo}
\date{Sesiones: 5, 6, 9}

\begin{document}
\maketitle

\begin{abstract}
Este documento presenta el proceso iterativo de optimización arquitectónica
del modelo de predicción de landmarks. Se documentan la implementación de
Coordinate Attention, la transición de BatchNorm a GroupNorm, la
optimización de la cabeza de regresión (hidden\_dim=768, dropout=0.3), y
la introducción de Test-Time Augmentation (TTA). El proceso redujo el error
de 9.08 píxeles (baseline) a 7.21 píxeles mediante ablación sistemática
de hiperparámetros. Se identifican y corrigen bugs críticos relacionados
con el congelamiento de parámetros y la normalización en lotes pequeños.
\end{abstract}

\tableofcontents
\newpage

% ==============================================================================
\section{Introducción y Objetivos}
% ==============================================================================

Tras el entrenamiento inicial, el objetivo de las
sesiones 5, 6 y 9 fue optimizar sistemáticamente la arquitectura mediante:

\begin{enumerate}
    \item Mejoras arquitectónicas (Coordinate Attention, cabeza profunda)
    \item Optimización de hiperparámetros (dropout, hidden dimensions)
    \item Técnicas de inferencia (Test-Time Augmentation)
    \item Corrección de bugs que limitaban el rendimiento
\end{enumerate}

\begin{table}[htbp]
\centering
\caption{Progreso del error a través de las sesiones de optimización}
\label{tab:progress_optimization}
\begin{tabular}{lcc}
\toprule
\textbf{Sesión} & \textbf{Error Test (px)} & \textbf{Mejora} \\
\midrule
Inicial & 9.08 & - \\
5 (TTA) & 8.80 & -3.1\% \\
6 (Bugs corregidos) & 7.80 & -14.1\% \\
9 (hidden\_dim=768) & 7.21 & -20.6\% \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Sesión 5: Coordinate Attention y TTA}
% ==============================================================================

\subsection{Coordinate Attention Module}

El módulo de Coordinate Attention fue propuesto en CVPR 2021 como una
alternativa más eficiente a Squeeze-and-Excitation que preserva información
posicional.

\begin{definicion}[Coordinate Attention]
Dado un feature map $X \in \R^{C \times H \times W}$, Coordinate Attention
genera mapas de atención direccionales que capturan dependencias de largo
alcance mientras preservan información posicional precisa.
\end{definicion}

\subsubsection{Proceso de Atención}

El proceso se divide en tres etapas:

\textbf{1. Pooling direccional:}
\begin{equation}
z_c^h(h) = \frac{1}{W} \sum_{j=0}^{W-1} x_c(h, j)
\label{eq:h_pool}
\end{equation}

\begin{equation}
z_c^w(w) = \frac{1}{H} \sum_{i=0}^{H-1} x_c(i, w)
\label{eq:w_pool}
\end{equation}

Esto genera dos vectores de características direccionales: $z^h \in \R^{C \times H \times 1}$
y $z^w \in \R^{C \times 1 \times W}$.

\textbf{2. Transformación compartida:}
\begin{equation}
f = \delta(F_1([z^h, z^w]))
\label{eq:shared_transform}
\end{equation}

donde $[\cdot, \cdot]$ denota concatenación espacial, $F_1$ es una convolución
$1 \times 1$ que reduce dimensionalidad, y $\delta$ es la activación no lineal
(BatchNorm + ReLU).

\textbf{3. Generación de pesos de atención:}
\begin{equation}
g^h = \sigma(F_h(f^h)), \quad g^w = \sigma(F_w(f^w))
\label{eq:attention_weights}
\end{equation}

\begin{equation}
y_c(i, j) = x_c(i, j) \cdot g_c^h(i) \cdot g_c^w(j)
\label{eq:attention_output}
\end{equation}

donde $\sigma$ es la función sigmoide.

\subsection{Implementación}

\begin{lstlisting}[language=Python, caption={Módulo Coordinate Attention}]
class CoordAttention(nn.Module):
    def __init__(self, in_channels, reduction=32):
        super().__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        mid_channels = max(8, in_channels // reduction)
        self.conv1 = nn.Conv2d(in_channels, mid_channels, 1)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        self.act = nn.ReLU(inplace=True)

        self.conv_h = nn.Conv2d(mid_channels, in_channels, 1)
        self.conv_w = nn.Conv2d(mid_channels, in_channels, 1)

    def forward(self, x):
        B, C, H, W = x.shape

        # Pooling direccional
        x_h = self.pool_h(x)  # (B, C, H, 1)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # (B, C, W, 1)

        # Concatenacion y transformacion
        y = torch.cat([x_h, x_w], dim=2)  # (B, C, H+W, 1)
        y = self.act(self.bn1(self.conv1(y)))

        # Separar y generar pesos
        x_h, x_w = torch.split(y, [H, W], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)

        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()

        return x * a_h * a_w
\end{lstlisting}

\subsection{Test-Time Augmentation (TTA)}

TTA mejora la robustez promediando predicciones de múltiples versiones
aumentadas de la imagen de entrada.

\begin{definicion}[Test-Time Augmentation]
Dada una imagen $I$ y un modelo $f$, TTA computa la predicción como:
\begin{equation}
\hat{y} = \frac{1}{|A|} \sum_{a \in A} T_a^{-1}(f(T_a(I)))
\label{eq:tta}
\end{equation}
donde $A$ es el conjunto de transformaciones y $T_a^{-1}$ es la transformación
inversa para las coordenadas.
\end{definicion}

\subsubsection{Transformaciones Usadas}

Para landmarks, solo se usa flip horizontal:
\begin{equation}
A = \{\text{identidad}, \text{flip horizontal}\}
\label{eq:tta_transforms}
\end{equation}

La transformación inversa para coordenadas tras flip horizontal es:
\begin{equation}
(x', y') = (1 - x, y)
\label{eq:flip_inverse}
\end{equation}

Además, se intercambian los índices de pares simétricos (ver Sección 4 del
documento de preprocesamiento).

\subsection{Resultados Sesión 5}

\begin{table}[htbp]
\centering
\caption{Experimentos de Sesión 5}
\label{tab:session5_results}
\begin{tabular}{lccc}
\toprule
\textbf{Configuración} & \textbf{Error Test} & \textbf{vs Baseline} & \textbf{Resultado} \\
\midrule
Baseline (Wing Loss) & 9.08 px & - & - \\
\textbf{Baseline + TTA} & \textbf{8.80 px} & -0.28 px & \checkmark \\
CoordAttn + DeepHead & 10.44 px & +1.36 px & $\times$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{hallazgo}[title={TTA mejora sin costo de entrenamiento}]
TTA reduce el error en 0.28 px (3.1\%) sin necesidad de re-entrenar el modelo.
Es una mejora ``gratuita'' que solo duplica el tiempo de inferencia.
\end{hallazgo}

\begin{observacion}
Coordinate Attention + DeepHead empeoró el resultado en esta sesión debido
a bugs que se identificaron y corrigieron en la Sesión 6.
\end{observacion}

% ==============================================================================
\section{Sesión 6: Corrección de Bugs Críticos}
% ==============================================================================

\subsection{Bug 1: CoordAttention en Phase 1}

\textbf{Problema}: Durante Phase 1 (backbone congelado), el módulo
CoordAttention se estaba entrenando junto con la cabeza, cuando debería
permanecer congelado como parte del extractor de características.

\textbf{Impacto}: Los pesos de CoordAttention se modificaban con gradientes
provenientes de una cabeza mal inicializada, corrompiendo las representaciones.

\textbf{Solución}: Implementar métodos separados para control granular:

\begin{lstlisting}[language=Python, caption={Métodos de congelamiento corregidos}]
def freeze_all_except_head(self):
    """Congela backbone Y coord_attention."""
    for param in self.features.parameters():
        param.requires_grad = False
    for param in self.coord_attention.parameters():
        param.requires_grad = False
    for param in self.head.parameters():
        param.requires_grad = True

def unfreeze_all(self):
    """Descongela todo para Phase 2."""
    for param in self.parameters():
        param.requires_grad = True
\end{lstlisting}

\subsection{Bug 2: BatchNorm con Batch Pequeño}

\textbf{Problema}: BatchNorm1d en la cabeza profunda era inestable con
batches pequeños (típico en datasets médicos limitados).

\textbf{Causa}: BatchNorm estima estadísticas de normalización del batch
actual. Con batches pequeños ($<32$), estas estimaciones tienen alta varianza,
causando inestabilidad durante el entrenamiento.

\textbf{Solución}: Reemplazar BatchNorm1d por GroupNorm:

\begin{equation}
\hat{x}_i = \frac{x_i - \mu_G}{\sqrt{\sigma_G^2 + \epsilon}}
\label{eq:groupnorm}
\end{equation}

donde $G$ es un grupo de canales (no el batch), haciendo la normalización
independiente del tamaño del batch.

\begin{lstlisting}[language=Python, caption={Cabeza con GroupNorm}]
self.head = nn.Sequential(
    nn.Linear(512, 512),
    nn.GroupNorm(32, 512),  # 32 grupos de 16 canales
    nn.ReLU(inplace=True),
    nn.Dropout(0.5),
    nn.Linear(512, hidden_dim),
    nn.GroupNorm(16, hidden_dim),  # Adaptativo
    nn.ReLU(inplace=True),
    nn.Dropout(0.5),
    nn.Linear(hidden_dim, 30),
    nn.Sigmoid()
)
\end{lstlisting}

\subsection{Resultados Post-Corrección}

\begin{table}[htbp]
\centering
\caption{Resultados tras corrección de bugs}
\label{tab:session6_results}
\begin{tabular}{lcc}
\toprule
\textbf{Modelo} & \textbf{Sin TTA} & \textbf{Con TTA} \\
\midrule
Baseline (código nuevo) & 9.87 px & 9.76 px \\
CoordAttn + DeepHead (corregido) & 9.45 px & \textbf{8.93 px} \\
\bottomrule
\end{tabular}
\end{table}

\begin{observacion}
Con bugs corregidos, CoordAttention + DeepHead supera al baseline por
0.83 px con TTA. La diferencia con Sesión 5 se atribuye a los bugs que
impedían el aprendizaje correcto.
\end{observacion}

\subsection{Análisis: Por qué Restricciones Geométricas No Ayudan}

Durante Sesión 6 se investigó por qué CentralAlignmentLoss y SoftSymmetryLoss
no mejoraban los resultados:

\begin{table}[htbp]
\centering
\caption{Análisis de restricciones geométricas}
\label{tab:geometric_analysis}
\begin{tabular}{lcc}
\toprule
\textbf{Restricción} & \textbf{GT} & \textbf{Predicciones} \\
\midrule
Central Alignment (distancia al eje) & 0.0044 & 0.0025 \\
Symmetry Loss & 0.0000 & 0.0000 \\
Contribución al loss total & - & 1-4\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{hallazgo}[title={Restricciones geométricas ya satisfechas}]
Las restricciones geométricas no aportan gradientes útiles porque:
\begin{enumerate}
    \item El ground truth ya satisface las restricciones (diseño del etiquetado)
    \item El modelo aprende a satisfacerlas implícitamente
    \item Los términos de loss geométricos contribuyen $<$5\% del total
\end{enumerate}
\end{hallazgo}

% ==============================================================================
\section{Sesión 9: Optimización de Hiperparámetros}
% ==============================================================================

\subsection{Motivación}

Tras CLAHE (Sesiones 7-8) que logró 7.84 px, la Sesión 9 exploró optimización
de hiperparámetros de la cabeza de regresión:

\begin{itemize}
    \item Dropout: ¿0.5 es óptimo o hay mejor valor?
    \item hidden\_dim: ¿256 es suficiente o se necesita más capacidad?
    \item CLAHE tile\_size: ¿Valores menores ayudan más?
\end{itemize}

\subsection{Experimentos de Ablación}

\begin{table}[htbp]
\centering
\caption{Ablación de hiperparámetros (Sesión 9)}
\label{tab:session9_ablation}
\begin{tabular}{llccc}
\toprule
\textbf{Exp.} & \textbf{Configuración} & \textbf{Error Test} & \textbf{$\Delta$} & \textbf{Nota} \\
\midrule
Baseline & Sesión 8 & 7.84 px & - & - \\
1 & CLAHE tile=2 & 7.88 px & +0.04 & No mejora \\
2 & CLAHE clip=1.0 & 7.85 px & +0.01 & No mejora \\
2b & CLAHE clip=1.5 & 8.38 px & +0.54 & Empeora \\
3 & \textbf{dropout=0.3} & 7.60 px & \textbf{-0.24} & \checkmark \\
\textbf{4} & \textbf{hidden\_dim=768} & \textbf{7.21 px} & \textbf{-0.63} & \checkmark\checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Análisis de Dropout}

\begin{proposicion}[Dropout Óptimo]
Dropout de 0.3 supera a dropout de 0.5 por 0.24 px. Menos regularización
permite mejor ajuste a los datos cuando:
\begin{enumerate}
    \item El dataset es pequeño pero las muestras son representativas
    \item Las transformaciones de augmentation ya proveen regularización
    \item No hay evidencia de overfitting severo
\end{enumerate}
\end{proposicion}

\subsection{Análisis de Hidden Dimension}

\begin{table}[htbp]
\centering
\caption{Efecto de hidden\_dim en la cabeza de regresión}
\label{tab:hidden_dim}
\begin{tabular}{lccc}
\toprule
\textbf{hidden\_dim} & \textbf{Parámetros Cabeza} & \textbf{Error Test} & \textbf{vs 256} \\
\midrule
256 (original) & 137K & 7.84 px & baseline \\
512 & 270K & 7.29 px & -7.0\% \\
\textbf{768} & \textbf{407K} & \textbf{7.21 px} & \textbf{-8.0\%} \\
1024 & 548K & 7.35 px & -6.3\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{hallazgo}[title={hidden\_dim=768 es óptimo}]
La cabeza de regresión con 768 dimensiones ocultas logra el mejor balance
entre capacidad y generalización. Valores mayores (1024) no mejoran y
valores menores (256, 512) tienen capacidad insuficiente.
\end{hallazgo}

\subsection{Resultados por Categoría}

\begin{table}[htbp]
\centering
\caption{Error por categoría con hidden\_dim=768}
\label{tab:session9_by_category}
\begin{tabular}{lccc}
\toprule
\textbf{Categoría} & \textbf{Antes} & \textbf{Después} & \textbf{Mejora} \\
\midrule
Normal & 7.00 px & \textbf{6.34 px} & -9.4\% \\
Viral & 7.98 px & 8.50 px & +6.5\% \\
COVID & 9.03 px & \textbf{7.79 px} & -13.7\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{observacion}
La cabeza más grande mejora significativamente COVID (-13.7\%) porque tiene
más capacidad para modelar patrones complejos en imágenes con consolidaciones
pulmonares. Viral empeora ligeramente pero se mantiene aceptable.
\end{observacion}

% ==============================================================================
\section{Configuración Final Optimizada}
% ==============================================================================

\begin{table}[htbp]
\centering
\caption{Configuración arquitectónica final}
\label{tab:final_config}
\begin{tabular}{lll}
\toprule
\textbf{Componente} & \textbf{Configuración} & \textbf{Justificación} \\
\midrule
Backbone & ResNet-18 (ImageNet) & Transfer learning efectivo \\
Atención & Coordinate Attention & Información posicional \\
Normalización & GroupNorm & Estable con batch pequeño \\
hidden\_dim & 768 & Balance capacidad/generalización \\
Dropout & 0.3 & Menos regularización, mejor ajuste \\
Activación salida & Sigmoid & Coords en $[0,1]$ \\
Loss & Wing Loss & Óptima para landmarks \\
CLAHE & clip=2.0, tile=4 & Realce local \\
TTA & Flip horizontal & +3\% mejora gratis \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Arquitectura Final}

\begin{equation}
\text{Model} = \text{Backbone} \circ \text{CoordAttn} \circ \text{GAP} \circ \text{Head}
\label{eq:final_arch}
\end{equation}

\begin{table}[htbp]
\centering
\caption{Flujo de dimensiones en arquitectura final}
\label{tab:dimensions_flow}
\begin{tabular}{llc}
\toprule
\textbf{Capa} & \textbf{Operación} & \textbf{Salida} \\
\midrule
Input & Imagen RGB & $224 \times 224 \times 3$ \\
ResNet-18 (layer4) & Convolución + ResBlocks & $7 \times 7 \times 512$ \\
CoordAttention & Atención direccional & $7 \times 7 \times 512$ \\
GAP & Pooling global & $512$ \\
FC1 + GroupNorm + ReLU + Drop & Capa oculta 1 & $512$ \\
FC2 + GroupNorm + ReLU + Drop & Capa oculta 2 & $768$ \\
FC3 + Sigmoid & Salida & $30$ \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Comparación de Progreso}
% ==============================================================================

\begin{table}[htbp]
\centering
\caption{Evolución del error a través de optimizaciones}
\label{tab:evolution}
\begin{tabular}{llcc}
\toprule
\textbf{Sesión} & \textbf{Cambio Principal} & \textbf{Error} & \textbf{Mejora Acum.} \\
\midrule
4 & Baseline (Wing Loss) & 9.08 px & - \\
5 & + TTA & 8.80 px & -3.1\% \\
6 & Bugs corregidos (GroupNorm) & 8.93 px & -1.7\% \\
7 & + CLAHE & 8.18 px & -9.9\% \\
8 & CLAHE tile=4 & 7.84 px & -13.7\% \\
9 & dropout=0.3, hidden=768 & 7.21 px & -20.6\% \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Figuras Sugeridas}
% ==============================================================================

\subsection{Figura 6.1: Módulo Coordinate Attention}
\textit{Descripción}: Diagrama mostrando:
\begin{itemize}
    \item Feature map de entrada $C \times H \times W$
    \item Pooling horizontal y vertical
    \item Concatenación y transformación compartida
    \item Generación de pesos de atención $g^h$, $g^w$
    \item Multiplicación element-wise
\end{itemize}

\subsection{Figura 6.2: Comparación BatchNorm vs GroupNorm}
\textit{Descripción}: Diagrama mostrando:
\begin{itemize}
    \item BatchNorm: normaliza a través del batch (inestable con N pequeño)
    \item GroupNorm: normaliza dentro de grupos de canales (independiente de N)
    \item Visualización de qué dimensiones se promedian en cada caso
\end{itemize}

\subsection{Figura 6.3: Ablación de Hiperparámetros}
\textit{Descripción}: Gráfico de barras agrupadas mostrando:
\begin{itemize}
    \item Eje X: Configuraciones (dropout, hidden\_dim, CLAHE)
    \item Eje Y: Error en píxeles
    \item Barra de referencia (baseline) destacada
\end{itemize}

\subsection{Figura 6.4: Efecto de hidden\_dim}
\textit{Descripción}: Gráfico de línea mostrando:
\begin{itemize}
    \item Eje X: hidden\_dim (256, 512, 768, 1024)
    \item Eje Y: Error test (px)
    \item Punto óptimo en 768 destacado
\end{itemize}

\subsection{Figura 6.5: Progreso del Error por Sesión}
\textit{Descripción}: Gráfico de cascada (waterfall) mostrando:
\begin{itemize}
    \item Barras descendentes por cada mejora
    \item Baseline (9.08 px) a final (7.21 px)
    \item Contribución de cada cambio etiquetada
\end{itemize}

% ==============================================================================
\section{Archivos Fuente}
% ==============================================================================

\begin{table}[htbp]
\centering
\caption{Archivos modificados durante optimización}
\label{tab:source_files}
\begin{tabular}{ll}
\toprule
\textbf{Archivo} & \textbf{Contenido} \\
\midrule
\archivo{src\_v2/models/resnet\_landmark.py} & CoordAttention, GroupNorm, head profunda \\
\archivo{src\_v2/evaluation/metrics.py} & TTA (predict\_with\_tta) \\
\archivo{scripts/train.py} & Flags --coord-attention, --deep-head, --tta \\
\archivo{src\_v2/training/trainer.py} & freeze\_all\_except\_head, unfreeze\_all \\
\bottomrule
\end{tabular}
\end{table}

% ==============================================================================
\section{Conclusiones}
% ==============================================================================

\begin{enumerate}
    \item \textbf{TTA es mejora sin costo}: Reduce error 3\% sin re-entrenar,
    solo duplicando tiempo de inferencia.

    \item \textbf{GroupNorm supera BatchNorm}: Con batches pequeños típicos
    en datasets médicos, GroupNorm es más estable.

    \item \textbf{Control granular de congelamiento es crítico}: El módulo de
    atención debe congelarse junto con el backbone en Phase 1.

    \item \textbf{Dropout óptimo es 0.3}: Menos regularización permite mejor
    ajuste cuando hay suficiente augmentation.

    \item \textbf{hidden\_dim=768 es óptimo}: Ni muy pequeño (subajuste) ni
    muy grande (sin mejora adicional).

    \item \textbf{Mejora total: 20.6\%}: De 9.08 px inicial a 7.21 px final
    mediante optimización sistemática del modelo individual.

    \item \textbf{Las restricciones geométricas no aportan}: El modelo
    aprende implícitamente la estructura geométrica del problema.
\end{enumerate}

\end{document}
