\section{Estrategia de Entrenamiento Progresivo}
\label{sec:estrategia_entrenamiento}

% Como se fundamentó teóricamente en la Sección~\ref{sec:funciones_perdida} del marco teórico, (NOTA: label no existe en Cap 2 actual)
La incorporación de conocimiento anatómico mediante restricciones geométricas diferenciables constituye un paradigma promisorio para mejorar consistencia estructural de predicciones de \textit{landmarks} en imágenes médicas. La estrategia de entrenamiento desarrollada en este trabajo implementa este paradigma mediante un protocolo progresivo en cuatro fases que incorpora gradualmente funciones de pérdida geométricamente restringidas, comenzando con optimización estándar mediante Error Cuadrático Medio (\textit{Mean Squared Error}, MSE) para establecer una línea base, transitando a \textit{Wing Loss} para mejorar precisión sub-píxel, agregando \textit{Symmetry Loss} para imponer consistencia bilateral, y finalmente incorporando \textit{Distance Preservation Loss} para garantizar proporciones anatómicas válidas. Cada fase se construye sobre la anterior mediante inicialización con pesos óptimos de la fase previa, estrategia de \textit{warm-start} (inicio cálido) que acelera convergencia y previene degradación de desempeño al introducir términos de pérdida adicionales.

Esta organización en fases progresivas, en lugar de entrenamiento directo con la función de pérdida completa desde el inicio, se fundamenta en observaciones empíricas previas sobre dificultad de optimización de funciones de pérdida multi-objetivo complejas: el entrenamiento simultáneo con múltiples términos de pérdida geométrica desde inicialización aleatoria frecuentemente resulta en inestabilidad numérica, convergencia prematura a mínimos locales de calidad inferior, o dificultad en balancear magnitudes relativas de gradientes provenientes de diferentes términos. La incorporación gradual permite al modelo primero establecer predicciones aproximadamente correctas mediante supervisión MSE estándar, luego refinar precisión mediante \textit{Wing Loss} que proporciona gradientes más informativos en régimen de error pequeño, posteriormente mejorar consistencia geométrica mediante \textit{Symmetry Loss}, y finalmente incorporar restricciones de proporciones anatómicas mediante \textit{Distance Preservation Loss}, secuencia que guía la optimización a través de paisaje de pérdida complejo de manera controlada.

\subsection{Fase 1: Entrenamiento del Módulo de Regresión con Backbone Congelado}
\label{sec:phase1_head_training}

La primera fase implementa el protocolo estándar de \textit{transfer learning} en dos etapas: congelar completamente los pesos del \textit{backbone} preentrenado y entrenar únicamente el módulo de regresión añadido, permitiendo que las capas superiores aprendan a mapear representaciones visuales de ImageNet a coordenadas de \textit{landmarks} anatómicos sin perturbar las características de bajo nivel ya aprendidas. Esta estrategia conservadora es particularmente apropiada cuando el \textit{dataset} objetivo es pequeño ($< 1000$ imágenes) y el riesgo de sobreajuste es alto: entrenar todos los 11.6 millones de parámetros desde inicialización aleatoria con solo 669 imágenes de entrenamiento resultaría inevitablemente en memorización de datos de entrenamiento sin capacidad de generalización.

El modelo de Fase 1 se define formalmente como $f_{\theta} = h_{\phi} \circ g_{\psi}$, donde $g_{\psi}: \mathbb{R}^{224 \times 224 \times 3} \to \mathbb{R}^{512}$ representa el \textit{backbone} ResNet-18 que mapea imágenes a vectores de características de 512 dimensiones con parámetros $\psi$ inicializados desde ImageNet y mantenidos fijos ($\nabla_{\psi} \mathcal{L} = 0$ forzado), y $h_{\phi}: \mathbb{R}^{512} \to \mathbb{R}^{30}$ representa el módulo de regresión de tres capas completamente conectadas con parámetros $\phi$ inicializados aleatoriamente mediante inicialización Kaiming \cite{He2015}, esquema que escala pesos iniciales según número de conexiones para garantizar estabilidad numérica durante propagación hacia adelante y retropropagación de gradientes.

\textbf{Configuración de Fase 1:}
\begin{itemize}
    \item \textbf{Función de pérdida:} MSE estándar sobre coordenadas normalizadas
    \begin{equation}
        \mathcal{L}_{MSE}(\hat{\mathbf{y}}, \mathbf{y}) = \frac{1}{30} \sum_{i=1}^{30} (\hat{y}_i - y_i)^2
        \label{eq:mse_loss_phase1}
    \end{equation}
    donde $\hat{\mathbf{y}} \in [0,1]^{30}$ son coordenadas predichas (salida Sigmoid) y $\mathbf{y} \in [0,1]^{30}$ son coordenadas \textit{ground truth} normalizadas.

    \item \textbf{Optimizador:} Adam \cite{Kingma2015} con parámetros estándar ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$)
    \item \textbf{Tasa de aprendizaje:} $\alpha = 1 \times 10^{-3}$ (constante durante toda la fase)
    \item \textbf{Regularización L2:} \textit{Weight decay} $\lambda = 1 \times 10^{-4}$ aplicado solo a parámetros $\phi$ del módulo de regresión
    \item \textbf{Tamaño de batch:} 32 imágenes (compromiso entre eficiencia computacional y estabilidad de gradientes)
    \item \textbf{Número de épocas:} 15 (suficiente para convergencia del módulo de regresión sin sobreajuste)
    \item \textbf{Parámetros entrenables:} $|\phi| \approx 400{,}000$ (solo módulo de regresión), correspondiente al 3.4\% del total
\end{itemize}

El entrenamiento de Fase 1 emplea MSE como función de pérdida inicial por simplicidad y estabilidad: MSE proporciona gradientes bien comportados sin singularidades, superficie de pérdida convexa localmente que facilita convergencia desde inicialización aleatoria, y minimización directa de discrepancia euclidiana entre predicciones y \textit{ground truth}. Aunque MSE presenta limitaciones conocidas para localización sub-píxel (penalización uniforme independiente de magnitud de error, sesgo hacia promedio de distribución en presencia de outliers), estas desventajas son menos críticas en fase inicial donde objetivo es establecer aproximación razonable al mapeo imágenes$\to$coordenadas antes de refinamientos posteriores.

El protocolo específico de Fase 1 procede mediante iteración sobre \textit{mini-batches} de 32 imágenes extraídos aleatoriamente del conjunto de entrenamiento, computando predicciones mediante propagación hacia adelante a través del modelo $\hat{\mathbf{y}}^{(b)} = f_{\theta}(\mathbf{X}^{(b)})$ donde $\mathbf{X}^{(b)} \in \mathbb{R}^{32 \times 3 \times 224 \times 224}$ es el \textit{batch} de imágenes preprocesadas, calculando pérdida MSE promediada sobre el \textit{batch} $\mathcal{L}^{(b)} = \frac{1}{32}\sum_{i=1}^{32} \mathcal{L}_{MSE}(\hat{\mathbf{y}}^{(b)}_i, \mathbf{y}^{(b)}_i)$, computando gradientes mediante retropropagación $\nabla_{\phi} \mathcal{L}^{(b)}$ (solo respecto a parámetros del módulo de regresión, gradientes del \textit{backbone} son descartados), y actualizando parámetros mediante regla de Adam que combina momentum de primer y segundo orden para convergencia estable. Después de procesar todos los \textit{mini-batches} del conjunto de entrenamiento (una época), el modelo se evalúa sobre el conjunto de validación para monitoreo de convergencia, guardando el \textit{checkpoint} (punto de control) del modelo con menor pérdida de validación observada hasta el momento.

El entrenamiento de Fase 1 típicamente completa en aproximadamente 1 minuto en la configuración de \textit{hardware} empleada (GPU AMD RX 6600), tiempo reducido explicado por el pequeño número de parámetros entrenables (400K vs 11.6M totales) y épocas limitadas (15), suficientes para convergencia del módulo de regresión sin necesitar ajuste fino extenso del \textit{backbone}. El modelo resultante establece una línea base funcional que captura correspondencia aproximada entre apariencia visual de radiografías y posiciones de \textit{landmarks}, aunque con precisión limitada debido a las limitaciones inherentes de MSE para localización sub-píxel, aspecto abordado en fases subsiguientes mediante funciones de pérdida especializadas.

\subsection{Fase 2: Fine-Tuning Completo con Wing Loss}
\label{sec:phase2_wing_loss}

La segunda fase desbloquea todos los parámetros del modelo, permitiendo optimización de la arquitectura completa mediante \textit{fine-tuning} que adapta representaciones visuales preentrenadas en ImageNet a características específicas de radiografías de tórax, simultáneamente introduciendo \textit{Wing Loss} como función de pérdida especializada para localización sub-píxel de \textit{landmarks}. % Como se derivó matemáticamente en la Sección~\ref{sec:wing_loss_teoria}, (NOTA: label no existe en Cap 2 actual)
\textit{Wing Loss} \cite{Feng2018} proporciona gradientes más informativos que MSE en régimen de error pequeño mediante transición suave entre comportamiento logarítmico cerca de error cero (gradiente grande, aceleración de convergencia final) y comportamiento lineal para errores grandes (robustez ante \textit{outliers}), característica demostrada empíricamente en detección de \textit{landmarks} faciales y extendida exitosamente a dominio médico en trabajos recientes.

El modelo de Fase 2 mantiene la arquitectura idéntica a Fase 1 ($f_{\theta} = h_{\phi} \circ g_{\psi}$), pero todos los parámetros $\theta = \{\psi, \phi\}$ son ahora entrenables con tasas de aprendizaje diferenciadas: el \textit{backbone} $\psi$ recibe tasa de aprendizaje reducida para preservar parcialmente conocimiento de ImageNet, mientras el módulo de regresión $\phi$ mantiene tasa de aprendizaje estándar para adaptación rápida. Esta estrategia de tasas de aprendizaje diferenciadas implementa el principio de \textit{discriminative learning rates} que reconoce que capas inferiores (características genéricas de bajo nivel) requieren ajuste mínimo, mientras capas superiores (características específicas de tarea) necesitan adaptación sustancial.

\textbf{Configuración de Fase 2:}
\begin{itemize}
    \item \textbf{Función de pérdida:} \textit{Wing Loss} con parámetros $\omega=10.0$, $\epsilon=2.0$
    \begin{equation}
        \mathcal{L}_{wing}(x) = \begin{cases}
            \omega \times \ln\left(1 + \frac{|x|}{\epsilon}\right) & \text{si } |x| < \omega \\
            |x| - C & \text{si } |x| \geq \omega
        \end{cases}
        \label{eq:wing_loss_implemented}
    \end{equation}
    donde $x = \hat{y}_i - y_i$ es el error de predicción por coordenada, $\omega = 10.0$ es el umbral de transición (expresado en escala normalizada $[0,1]$, equivalente a $\approx 2.24$ píxeles en imagen de 224$\times$224), $\epsilon = 2.0$ controla curvatura en régimen logarítmico, y $C = \omega - \omega\ln(1 + \omega/\epsilon) \approx 3.906$ es constante de continuidad. La pérdida total se promedia sobre las 30 coordenadas:
    \begin{equation}
        \mathcal{L}_{total}^{(P2)} = \frac{1}{30} \sum_{i=1}^{30} \mathcal{L}_{wing}(\hat{y}_i - y_i)
        \label{eq:total_loss_phase2}
    \end{equation}

    \item \textbf{Optimizador:} Adam con grupos de parámetros separados
    \begin{itemize}
        \item Parámetros \textit{backbone} $\psi$: $\alpha_{back} = 2 \times 10^{-5}$ (tasa reducida 50×)
        \item Parámetros módulo regresión $\phi$: $\alpha_{head} = 2 \times 10^{-4}$ (tasa estándar)
    \end{itemize}
    \item \textbf{Regularización L2:} \textit{Weight decay} $\lambda = 5 \times 10^{-5}$ (reducido vs Fase 1 para permitir mayor flexibilidad)
    \item \textbf{Scheduler de tasa de aprendizaje:} CosineAnnealingLR \cite{Loshchilov2017} con período $T_{max}=70$ épocas y tasa mínima $\eta_{min} = 2 \times 10^{-6}$, implementando decaimiento suave según
    \begin{equation}
        \alpha_t = \eta_{min} + \frac{1}{2}(\alpha_0 - \eta_{min})\left(1 + \cos\left(\frac{t\pi}{T_{max}}\right)\right)
        \label{eq:cosine_annealing}
    \end{equation}
    donde $t$ es el número de época actual y $\alpha_0$ es la tasa de aprendizaje inicial ($\alpha_{back}$ o $\alpha_{head}$ según grupo de parámetros). Este \textit{scheduler} proporciona decaimiento gradual que facilita convergencia a mínimos de alta calidad.

    \item \textbf{Tamaño de batch:} 8 imágenes (reducido vs Fase 1 para estabilidad al optimizar 11.6M parámetros)
    \item \textbf{Número de épocas:} 70 (entrenamiento extenso para convergencia completa)
    \item \textbf{Early stopping:} Paciencia de 15 épocas sin mejora en pérdida de validación, deteniendo entrenamiento anticipadamente si el modelo deja de mejorar
    \item \textbf{Inicialización:} Pesos $\theta^{init}_{P2} = \theta^{best}_{P1}$ (\textit{warm-start} desde mejor \textit{checkpoint} de Fase 1)
    \item \textbf{Parámetros entrenables:} $|\theta| = 11{,}578{,}206$ (arquitectura completa)
\end{itemize}

La selección de \textit{Wing Loss} sobre alternativas como L1 o Smooth L1 se fundamenta en su comportamiento de gradiente adaptativo: para errores pequeños ($|x| < \omega$), el gradiente $\partial \mathcal{L}_{wing}/\partial x = \frac{\omega}{\epsilon + |x|} \cdot \text{sign}(x)$ escala inversamente con error, proporcionando fuerza mayor cuando la predicción está muy cerca del \textit{ground truth}, acelerando convergencia final a precisión sub-píxel. Para errores grandes ($|x| \geq \omega$), el gradiente se satura a $\text{sign}(x)$, proporcionando robustez ante \textit{outliers} similar a L1. Los valores de hiperparámetros $\omega=10.0$ y $\epsilon=2.0$ fueron establecidos por Feng et al. \cite{Feng2018} basándose en experimentos extensos en detección facial y adoptados aquí sin modificación, constituyendo configuración estándar en literatura de localización de \textit{landmarks}.

El protocolo de Fase 2 implementa entrenamiento estándar con dos grupos de parámetros, donde el optimizador Adam mantiene estadísticas de momentum separadas para cada grupo y aplica tasas de aprendizaje diferenciadas. La reducción de tamaño de \textit{batch} de 32 a 8 es necesaria por limitaciones de memoria GPU (8GB VRAM) al propagar gradientes a través de toda la arquitectura ResNet-18, aunque \textit{batch} de 8 mantiene estimación de gradiente suficientemente estable mediante acumulación de estadísticas de momentum de Adam. El \textit{scheduler} CosineAnnealingLR proporciona decaimiento suave de tasa de aprendizaje que evita oscilaciones en fases finales de entrenamiento, transicionando gradualmente de exploración con tasa alta a refinamiento con tasa baja.

La estrategia de \textit{early stopping} monitorea pérdida de validación después de cada época, manteniendo registro del mejor valor observado y contador de épocas sin mejora. Cuando el contador alcanza paciencia de 15 épocas (aproximadamente 20\% del máximo de 70 épocas), el entrenamiento se detiene anticipadamente, previniendo sobreajuste prolongado al conjunto de entrenamiento. El modelo final de Fase 2 corresponde al checkpoint con menor pérdida de validación, no al modelo de la última época, implementando principio de selección de modelo basada en desempeño de generalización en lugar de ajuste a entrenamiento.

\subsection{Fase 3: Incorporación de Symmetry Loss para Consistencia Bilateral}
\label{sec:phase3_symmetry}

La tercera fase introduce restricciones de simetría bilateral mediante \textit{Symmetry Loss}, función de pérdida geométrica que penaliza inconsistencias entre posiciones de \textit{landmarks} pareados a través del eje mediastínico. Como se fundamentó en la Sección~\ref{subsec:simetria_bilateral}, la simetría bilateral es una invariante anatómica de la estructura torácica que puede explotarse como supervisión adicional: pares de \textit{landmarks} correspondientes a estructuras izquierda-derecha (ápices, hila, bases pulmonares) deben presentar reflexión aproximada respecto al eje vertical definido por el mediastino, restricción que proporciona señal de aprendizaje complementaria a la supervisión de coordenadas punto-a-punto.

La función \textit{Symmetry Loss} implementada compara posiciones predichas de pares simétricos con sus reflexiones esperadas, calculando discrepancia mediante distancia euclidiana. Para cada par $(i,j) \in \mathcal{P}_{sym}$ definido en Ecuación~\ref{eq:pares_simetricos}, se computa la posición esperada del \textit{landmark} derecho $j$ como reflexión del \textit{landmark} izquierdo $i$ a través del eje mediastínico $x = x_{axis}$ dado por Ecuación~\ref{eq:eje_simetria}, y se penaliza desviación de esta predicción. La pérdida se formula bidireccionalmente (izquierda$\to$derecha y derecha$\to$izquierda) para tratar ambos lados simétricamente:

\begin{equation}
\mathcal{L}_{symmetry}(\hat{\mathbf{y}}) = \frac{1}{2|\mathcal{P}_{sym}|} \sum_{(i,j) \in \mathcal{P}_{sym}} \left[ \left\| \hat{\mathbf{p}}_j - \text{Mirror}(\hat{\mathbf{p}}_i, x_{axis}) \right\|_2 + \left\| \hat{\mathbf{p}}_i - \text{Mirror}(\hat{\mathbf{p}}_j, x_{axis}) \right\|_2 \right]
\label{eq:symmetry_loss_phase3}
\end{equation}

donde $\hat{\mathbf{p}}_k = [\hat{x}_k, \hat{y}_k]^T$ representa las coordenadas 2D predichas del \textit{landmark} $k$, $|\mathcal{P}_{sym}| = 5$ es el número de pares simétricos, y la operación de reflexión especular se define como:

\begin{equation}
\text{Mirror}(\mathbf{p}, x_{axis}) = \begin{bmatrix} 2x_{axis} - p_x \\ p_y \end{bmatrix}
\label{eq:mirror_operation}
\end{equation}

reflejando la coordenada horizontal a través de $x = x_{axis}$ mientras preservando la coordenada vertical. El eje $x_{axis}$ se calcula dinámicamente para cada predicción usando Ecuación~\ref{eq:eje_simetria} con las coordenadas predichas de \textit{landmarks} mediastínicos, permitiendo que el eje de simetría se adapte a la imagen específica en lugar de asumir eje fijo en el centro de la imagen, lo cual sería inadecuado para radiografías con rotación o descentrado del paciente.

La función de pérdida total de Fase 3 combina \textit{Wing Loss} con \textit{Symmetry Loss} mediante suma ponderada:

\begin{equation}
\mathcal{L}_{total}^{(P3)} = \mathcal{L}_{wing} + \lambda_{sym} \cdot \mathcal{L}_{symmetry}
\label{eq:total_loss_phase3}
\end{equation}

donde $\lambda_{sym} = 0.3$ es el peso de simetría, seleccionado mediante validación para balancear contribución de restricción geométrica con supervisión de coordenadas directa. El peso $\lambda_{sym} < 1$ indica que \textit{Symmetry Loss} actúa como regularizador que guía predicciones hacia configuraciones anatómicamente consistentes sin dominar la optimización.

\textbf{Configuración de Fase 3:}
\begin{itemize}
    \item \textbf{Función de pérdida:} Combinación \textit{Wing Loss} + \textit{Symmetry Loss} (Ecuación~\ref{eq:total_loss_phase3})
    \item \textbf{Peso de simetría:} $\lambda_{sym} = 0.3$
    \item \textbf{Optimizador, tasas de aprendizaje, \textit{batch size}, \textit{scheduler}:} Idénticos a Fase 2
    \item \textbf{Número de épocas:} 70 (mismo que Fase 2)
    \item \textbf{Early stopping:} Paciencia 15 épocas
    \item \textbf{Inicialización:} $\theta^{init}_{P3} = \theta^{best}_{P2}$ (\textit{warm-start} desde mejor \textit{checkpoint} de Fase 2)
\end{itemize}

La inicialización desde Fase 2 es crítica: comenzar Fase 3 desde el modelo que ya optimiza \textit{Wing Loss} efectivamente permite que \textit{Symmetry Loss} actúe como refinamiento incremental que mejora consistencia geométrica sin necesitar re-aprender mapeo básico imagen$\to$coordenadas. El entrenamiento de Fase 3 típicamente converge más rápidamente que Fase 2 (el \textit{early stopping} frecuentemente termina antes de 70 épocas) debido a la inicialización de alta calidad y naturaleza de refinamiento de la optimización.

\subsection{Fase 4: Complete Loss con Preservación de Distancias Anatómicas}
\label{sec:phase4_complete}

La cuarta y última fase incorpora \textit{Distance Preservation Loss}, función de pérdida que penaliza distorsiones de proporciones anatómicas mediante preservación de distancias euclidianas entre pares específicos de \textit{landmarks} que definen medidas estructurales críticas: altura mediastínica vertical, ancho torácico superior (ápices), ancho torácico medio (hila), ancho torácico inferior (bases). La preservación de estas distancias garantiza que el modelo no solo localice \textit{landmarks} individualmente con precisión, sino que mantenga relaciones geométricas globales consistentes con proporciones anatómicas humanas válidas.

\textit{Distance Preservation Loss} compara distancias euclidianas entre pares de \textit{landmarks} en predicciones con distancias correspondientes en \textit{ground truth}, penalizando discrepancias mediante pérdida L1 sobre diferencias de distancias:

\begin{equation}
\mathcal{L}_{distance}(\hat{\mathbf{y}}, \mathbf{y}) = \frac{1}{|\mathcal{P}_{dist}|} \sum_{(k,\ell) \in \mathcal{P}_{dist}} \left| \left\| \hat{\mathbf{p}}_k - \hat{\mathbf{p}}_\ell \right\|_2 - \left\| \mathbf{p}_k - \mathbf{p}_\ell \right\|_2 \right|
\label{eq:distance_loss}
\end{equation}

donde $\mathcal{P}_{dist}$ es el conjunto de pares de \textit{landmarks} cuyas distancias mutuas deben preservarse:

\begin{equation}
\mathcal{P}_{dist} = \{(0,1),\, (8,9),\, (2,3),\, (4,5),\, (6,7)\}
\label{eq:pares_distancia}
\end{equation}

correspondiendo a: $(0,1)$ altura mediastínica superior, $(8,9)$ eje mediastínico central, $(2,3)$ ancho torácico superior entre ápices, $(4,5)$ ancho torácico medio entre hila, y $(6,7)$ ancho torácico inferior entre bases. La norma L1 en la pérdida proporciona robustez ante \textit{outliers}: distancias individuales anómalas contribuyen linealmente a la pérdida total en lugar de cuadráticamente como en L2, reduciendo influencia de casos patológicos extremos con proporciones anatómicas genuinamente inusuales.

La función de pérdida completa de Fase 4 combina los tres términos mediante suma ponderada:

\begin{equation}
\mathcal{L}_{total}^{(P4)} = \mathcal{L}_{wing} + \lambda_{sym} \cdot \mathcal{L}_{symmetry} + \lambda_{dist} \cdot \mathcal{L}_{distance}
\label{eq:complete_loss}
\end{equation}

donde $\lambda_{sym} = 0.3$ (preservado desde Fase 3) y $\lambda_{dist} = 0.2$ es el peso de preservación de distancias, seleccionado para ser menor que $\lambda_{sym}$ reconociendo que restricciones de distancia son complementarias y menos críticas que restricciones de simetría bilateral para anatomía torácica.

\textbf{Configuración de Fase 4:}
\begin{itemize}
    \item \textbf{Función de pérdida:} \textit{Complete Loss} (Ecuación~\ref{eq:complete_loss})
    \item \textbf{Pesos de restricciones:} $\lambda_{sym} = 0.3$, $\lambda_{dist} = 0.2$
    \item \textbf{Optimizador, tasas de aprendizaje, \textit{batch size}, \textit{scheduler}:} Idénticos a Fases 2-3
    \item \textbf{Número de épocas:} 70
    \item \textbf{Early stopping:} Paciencia 15 épocas
    \item \textbf{Inicialización:} $\theta^{init}_{P4} = \theta^{best}_{P3}$ (\textit{warm-start} desde mejor \textit{checkpoint} de Fase 3)
\end{itemize}

La Fase 4 representa la culminación del entrenamiento progresivo, donde el modelo optimiza simultáneamente precisión de localización punto-a-punto (\textit{Wing Loss}), consistencia bilateral (\textit{Symmetry Loss}), y validez de proporciones anatómicas (\textit{Distance Preservation Loss}). La inicialización desde Fase 3 asegura que el modelo ya satisface restricciones de simetría razonablemente bien al comenzar Fase 4, permitiendo que \textit{Distance Preservation Loss} actúe como refinamiento final que mejora coherencia geométrica global sin desestabilizar convergencia.

\subsection{Estrategia de Warm-Start entre Fases}
\label{subsec:warm_start}

La estrategia de \textit{warm-start} constituye componente esencial del protocolo de entrenamiento progresivo: cada fase se inicializa con los pesos del mejor modelo de la fase inmediatamente anterior, implementando transferencia de conocimiento entre fases que acelera convergencia y previene degradación de desempeño al introducir términos de pérdida adicionales. Formalmente, la inicialización de Fase $k+1$ se define como:

\begin{equation}
\theta^{init}_{P_{k+1}} = \theta^{best}_{P_k}
\label{eq:warm_start}
\end{equation}

donde $\theta^{best}_{P_k} = \arg\min_{\theta} \mathcal{L}_{val}^{(P_k)}(\theta)$ es el conjunto de parámetros que minimizó pérdida de validación durante Fase $k$.

Esta estrategia contrasta con alternativas de entrenamiento desde inicialización aleatoria para cada fase o entrenamiento directo con función de pérdida completa desde el inicio. El entrenamiento desde inicialización aleatoria descartaría todo el conocimiento aprendido en fases previas, requiriendo que cada fase re-aprenda mapeo básico imagen$\to$coordenadas además de optimizar función de pérdida nueva, proceso ineficiente y propenso a convergencia a mínimos locales de calidad inferior. El entrenamiento directo con pérdida completa presenta dificultades de optimización multi-objetivo: los tres términos de pérdida tienen magnitudes y paisajes de gradiente diferentes, y optimización simultánea desde inicialización aleatoria frecuentemente resulta en balance subóptimo donde un término domina gradientes, previniendo que otros términos contribuyan efectivamente al aprendizaje.

La estrategia de \textit{warm-start} progresivo resuelve estos problemas mediante secuenciación cuidadosa: Fase 1 establece aproximación básica mediante MSE simple; Fase 2 refina precisión con \textit{Wing Loss} mientras preserva conocimiento de Fase 1; Fase 3 mejora consistencia geométrica con \textit{Symmetry Loss} sin degradar precisión de Fase 2; y Fase 4 incorpora proporciones anatómicas con \textit{Distance Loss} manteniendo beneficios de fases 2-3. Cada transición representa perturbación incremental de función de pérdida en lugar de cambio abrupto, facilitando adaptación suave del modelo a criterio de optimización progresivamente más complejo.

El protocolo completo de entrenamiento progresivo desde inicialización ImageNet hasta modelo final con \textit{Complete Loss} se resume como:

\begin{equation}
\text{ImageNet} \xrightarrow{\text{Fase 1: MSE, head only}} \theta^{best}_{P1} \xrightarrow{\text{Fase 2: Wing}} \theta^{best}_{P2} \xrightarrow{\text{Fase 3: +Symmetry}} \theta^{best}_{P3} \xrightarrow{\text{Fase 4: +Distance}} \theta^{best}_{P4}
\label{eq:training_pipeline}
\end{equation}

donde $\theta^{best}_{P4}$ constituye el modelo final empleado para evaluación sobre conjunto de prueba y análisis de desempeño presentado en el Capítulo~\ref{cap:resultados}.

La siguiente sección documenta detalles técnicos de implementación, incluyendo \textit{frameworks} de software, configuración de \textit{hardware}, tiempos de entrenamiento, y protocolos de reproducibilidad que permiten replicación independiente de la metodología descrita.
