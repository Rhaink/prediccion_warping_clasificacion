% ==============================================================================
% APÉNDICE A - DERIVACIONES MATEMÁTICAS COMPLETAS
% Proyecto: Detección de COVID-19 mediante Landmarks Anatómicos
% Nivel: Doctoral/Científico - Completo y Detallado
% ==============================================================================

\documentclass[12pt,a4paper]{article}
\input{00_preambulo}

\title{Apéndice A:\\
Derivaciones Matemáticas Completas}
\author{Documentación del Proceso de Desarrollo}
\date{Apéndice Técnico}

\begin{document}
\maketitle

\begin{abstract}
Este apéndice presenta las derivaciones matemáticas completas de los
algoritmos utilizados en el proyecto. Se incluyen las demostraciones
formales del análisis Procrustes, el algoritmo GPA, las transformaciones
afines, el warping piecewise affine con coordenadas baricéntricas, y
los fundamentos de las funciones de pérdida utilizadas en el entrenamiento
de redes neuronales.
\end{abstract}

\tableofcontents
\newpage

% ==============================================================================
\section{Análisis Procrustes}
% ==============================================================================

\subsection{Problema de Optimización}

Dadas dos configuraciones de $k$ landmarks en $m$ dimensiones:
\begin{itemize}
    \item $\mathbf{X} \in \mathbb{R}^{k \times m}$: Configuración de referencia
    \item $\mathbf{Y} \in \mathbb{R}^{k \times m}$: Configuración a alinear
\end{itemize}

El problema de superposición Procrustes busca:

\begin{equation}
\min_{s, \mathbf{R}, \mathbf{t}} \|\mathbf{X} - s\mathbf{Y}\mathbf{R} - \mathbf{1}\mathbf{t}^T\|_F^2
\label{eq:procrustes_problem}
\end{equation}

donde:
\begin{itemize}
    \item $s > 0$: Factor de escala
    \item $\mathbf{R} \in SO(m)$: Matriz de rotación ($\mathbf{R}^T\mathbf{R} = \mathbf{I}$, $\det(\mathbf{R}) = 1$)
    \item $\mathbf{t} \in \mathbb{R}^m$: Vector de traslación
    \item $\|\cdot\|_F$: Norma de Frobenius
\end{itemize}

\subsection{Solución para Traslación}

Expandiendo la función objetivo:

\begin{equation}
f(\mathbf{t}) = \|\mathbf{X} - s\mathbf{Y}\mathbf{R} - \mathbf{1}\mathbf{t}^T\|_F^2
\end{equation}

Derivando respecto a $\mathbf{t}$:

\begin{align}
\frac{\partial f}{\partial \mathbf{t}} &= -2(\mathbf{X} - s\mathbf{Y}\mathbf{R} - \mathbf{1}\mathbf{t}^T)^T \mathbf{1} \\
&= -2\mathbf{X}^T\mathbf{1} + 2s\mathbf{R}^T\mathbf{Y}^T\mathbf{1} + 2k\mathbf{t}
\end{align}

Igualando a cero:

\begin{equation}
\mathbf{t}^* = \frac{1}{k}(\mathbf{X}^T\mathbf{1} - s\mathbf{R}^T\mathbf{Y}^T\mathbf{1}) = \bar{\mathbf{x}} - s\mathbf{R}^T\bar{\mathbf{y}}
\end{equation}

donde $\bar{\mathbf{x}} = \frac{1}{k}\mathbf{X}^T\mathbf{1}$ es el centroide.

\begin{observacion}[Centrado previo]
Si centramos las configuraciones ($\bar{\mathbf{x}} = \bar{\mathbf{y}} = \mathbf{0}$),
entonces $\mathbf{t}^* = \mathbf{0}$ y el problema se reduce a encontrar $s$ y $\mathbf{R}$.
\end{observacion}

\subsection{Solución para Rotación mediante SVD}

Asumiendo configuraciones centradas, el problema se reduce a:

\begin{equation}
\min_{s, \mathbf{R}} \|\mathbf{X} - s\mathbf{Y}\mathbf{R}\|_F^2
\end{equation}

Expandiendo:

\begin{align}
\|\mathbf{X} - s\mathbf{Y}\mathbf{R}\|_F^2 &= \text{tr}[(\mathbf{X} - s\mathbf{Y}\mathbf{R})^T(\mathbf{X} - s\mathbf{Y}\mathbf{R})] \\
&= \text{tr}(\mathbf{X}^T\mathbf{X}) - 2s\text{tr}(\mathbf{R}^T\mathbf{Y}^T\mathbf{X}) + s^2\text{tr}(\mathbf{Y}^T\mathbf{Y})
\label{eq:expanded_objective}
\end{align}

\subsubsection{Teorema de la Rotación Óptima}

\begin{teorema}[Rotación óptima vía SVD]
Sea $\mathbf{Y}^T\mathbf{X} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$ la descomposición en valores singulares.
La rotación óptima es:
\begin{equation}
\mathbf{R}^* = \mathbf{V}\mathbf{U}^T
\end{equation}
\end{teorema}

\begin{proof}
De la ecuación~\eqref{eq:expanded_objective}, minimizar requiere maximizar
$\text{tr}(\mathbf{R}^T\mathbf{Y}^T\mathbf{X})$.

Sea $\mathbf{M} = \mathbf{Y}^T\mathbf{X}$ y su SVD $\mathbf{M} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T$.

Entonces:
\begin{align}
\text{tr}(\mathbf{R}^T\mathbf{M}) &= \text{tr}(\mathbf{R}^T\mathbf{U}\mathbf{\Sigma}\mathbf{V}^T) \\
&= \text{tr}(\mathbf{V}^T\mathbf{R}^T\mathbf{U}\mathbf{\Sigma}) \quad \text{(propiedad cíclica de traza)} \\
&= \text{tr}(\mathbf{Z}\mathbf{\Sigma})
\end{align}

donde $\mathbf{Z} = \mathbf{V}^T\mathbf{R}^T\mathbf{U}$.

Como $\mathbf{R}$, $\mathbf{U}$, y $\mathbf{V}$ son ortogonales, $\mathbf{Z}$ también es ortogonal.

Para una matriz ortogonal $\mathbf{Z}$, cada elemento satisface $|z_{ij}| \leq 1$.

Por lo tanto:
\begin{equation}
\text{tr}(\mathbf{Z}\mathbf{\Sigma}) = \sum_{i=1}^m z_{ii}\sigma_i \leq \sum_{i=1}^m \sigma_i
\end{equation}

El máximo se alcanza cuando $\mathbf{Z} = \mathbf{I}$, es decir:
\begin{equation}
\mathbf{V}^T\mathbf{R}^T\mathbf{U} = \mathbf{I} \implies \mathbf{R}^* = \mathbf{U}\mathbf{V}^T
\end{equation}

Nota: Si $\det(\mathbf{U}\mathbf{V}^T) = -1$ (reflexión), se usa:
\begin{equation}
\mathbf{R}^* = \mathbf{U}\text{diag}(1, ..., 1, -1)\mathbf{V}^T
\end{equation}
para garantizar una rotación propia.
\end{proof}

\subsection{Solución para Escala}

Sustituyendo $\mathbf{R}^*$ en la ecuación~\eqref{eq:expanded_objective} y derivando
respecto a $s$:

\begin{equation}
\frac{\partial}{\partial s}\left[\|\mathbf{X}\|_F^2 - 2s\text{tr}(\mathbf{R}^{*T}\mathbf{Y}^T\mathbf{X}) + s^2\|\mathbf{Y}\|_F^2\right] = 0
\end{equation}

\begin{equation}
-2\text{tr}(\mathbf{R}^{*T}\mathbf{Y}^T\mathbf{X}) + 2s\|\mathbf{Y}\|_F^2 = 0
\end{equation}

Por lo tanto:
\begin{equation}
s^* = \frac{\text{tr}(\mathbf{R}^{*T}\mathbf{Y}^T\mathbf{X})}{\|\mathbf{Y}\|_F^2} = \frac{\sum_i \sigma_i}{\|\mathbf{Y}\|_F^2}
\end{equation}

donde $\sigma_i$ son los valores singulares de $\mathbf{Y}^T\mathbf{X}$.

% ==============================================================================
\section{Análisis Generalizado de Procrustes (GPA)}
% ==============================================================================

\subsection{Formulación del Problema}

Dadas $n$ configuraciones de $k$ landmarks: $\{\mathbf{X}_1, ..., \mathbf{X}_n\}$,
el GPA busca:

\begin{equation}
\min_{\{s_i, \mathbf{R}_i, \mathbf{t}_i\}_{i=1}^n} \sum_{i=1}^{n} \|s_i\mathbf{X}_i\mathbf{R}_i + \mathbf{1}\mathbf{t}_i^T - \bar{\mathbf{X}}\|_F^2
\label{eq:gpa_objective}
\end{equation}

donde $\bar{\mathbf{X}} = \frac{1}{n}\sum_{i=1}^n (s_i\mathbf{X}_i\mathbf{R}_i + \mathbf{1}\mathbf{t}_i^T)$
es el consenso (forma media).

\subsection{Algoritmo Iterativo}

\begin{algorithm}
\caption{Análisis Generalizado de Procrustes}
\label{alg:gpa_complete}
\begin{algorithmic}[1]
\REQUIRE $\{\mathbf{X}_i\}_{i=1}^n$: Configuraciones de landmarks
\REQUIRE $\epsilon$: Tolerancia de convergencia
\ENSURE $\bar{\mathbf{X}}$: Forma canónica
\ENSURE $\{s_i, \mathbf{R}_i, \mathbf{t}_i\}$: Transformaciones óptimas

\STATE \textbf{Inicialización:}
\FOR{$i = 1$ to $n$}
    \STATE $\mathbf{X}_i \leftarrow \mathbf{X}_i - \mathbf{1}\bar{\mathbf{x}}_i^T$ \COMMENT{Centrar}
    \STATE $\mathbf{X}_i \leftarrow \mathbf{X}_i / \|\mathbf{X}_i\|_F$ \COMMENT{Normalizar escala}
\ENDFOR
\STATE $\bar{\mathbf{X}} \leftarrow \mathbf{X}_1$ \COMMENT{Referencia inicial}
\STATE $\text{error}_{\text{prev}} \leftarrow \infty$

\REPEAT
    \STATE \textbf{Paso 1: Alinear cada forma al consenso actual}
    \FOR{$i = 1$ to $n$}
        \STATE $\mathbf{M}_i \leftarrow \mathbf{X}_i^T \bar{\mathbf{X}}$
        \STATE $\mathbf{U}_i\mathbf{\Sigma}_i\mathbf{V}_i^T \leftarrow \text{SVD}(\mathbf{M}_i)$
        \STATE $\mathbf{R}_i \leftarrow \mathbf{V}_i\mathbf{U}_i^T$
        \IF{$\det(\mathbf{R}_i) < 0$}
            \STATE $\mathbf{R}_i \leftarrow \mathbf{V}_i\text{diag}(1, -1)\mathbf{U}_i^T$
        \ENDIF
        \STATE $\mathbf{X}_i \leftarrow \mathbf{X}_i\mathbf{R}_i$
    \ENDFOR

    \STATE \textbf{Paso 2: Calcular nuevo consenso}
    \STATE $\bar{\mathbf{X}} \leftarrow \frac{1}{n}\sum_{i=1}^n \mathbf{X}_i$

    \STATE \textbf{Paso 3: Normalizar consenso}
    \STATE $\bar{\mathbf{X}} \leftarrow \bar{\mathbf{X}} / \|\bar{\mathbf{X}}\|_F$

    \STATE \textbf{Paso 4: Calcular error de alineación}
    \STATE $\text{error} \leftarrow \frac{1}{n}\sum_{i=1}^n \|\mathbf{X}_i - \bar{\mathbf{X}}\|_F^2$

\UNTIL{$|\text{error}_{\text{prev}} - \text{error}| < \epsilon$}

\RETURN $\bar{\mathbf{X}}$, $\{s_i, \mathbf{R}_i, \mathbf{t}_i\}$
\end{algorithmic}
\end{algorithm}

\subsection{Convergencia del GPA}

\begin{teorema}[Convergencia del GPA]
El algoritmo GPA converge a un mínimo local de la función objetivo
\eqref{eq:gpa_objective}.
\end{teorema}

\begin{proof}
Definimos la función objetivo:
\begin{equation}
J = \sum_{i=1}^{n} \|\mathbf{X}_i^{(t)} - \bar{\mathbf{X}}^{(t)}\|_F^2
\end{equation}

\textbf{Paso 1}: La alineación Procrustes minimiza $\|\mathbf{X}_i\mathbf{R}_i - \bar{\mathbf{X}}\|_F^2$
para cada $i$, por lo tanto:
\begin{equation}
\|\mathbf{X}_i^{(t+1)} - \bar{\mathbf{X}}^{(t)}\|_F^2 \leq \|\mathbf{X}_i^{(t)} - \bar{\mathbf{X}}^{(t)}\|_F^2
\end{equation}

\textbf{Paso 2}: El nuevo consenso $\bar{\mathbf{X}}^{(t+1)} = \frac{1}{n}\sum_i \mathbf{X}_i^{(t+1)}$
es el promedio que minimiza la suma de distancias cuadradas:
\begin{equation}
\sum_{i=1}^{n} \|\mathbf{X}_i^{(t+1)} - \bar{\mathbf{X}}^{(t+1)}\|_F^2 \leq \sum_{i=1}^{n} \|\mathbf{X}_i^{(t+1)} - \bar{\mathbf{X}}^{(t)}\|_F^2
\end{equation}

Combinando:
\begin{equation}
J^{(t+1)} \leq J^{(t)}
\end{equation}

Como $J \geq 0$ y es monótonamente decreciente, el algoritmo converge.
\end{proof}

\subsection{Análisis de Componentes Principales de Forma}

Después de GPA, el análisis de variabilidad de forma usa PCA:

\begin{equation}
\mathbf{D} = \frac{1}{n-1}\sum_{i=1}^{n}(\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T
\end{equation}

donde $\mathbf{x}_i \in \mathbb{R}^{2k}$ es la vectorización de $\mathbf{X}_i$.

Los eigenvectores de $\mathbf{D}$ representan los modos principales de variación:

\begin{equation}
\mathbf{D}\mathbf{v}_j = \lambda_j\mathbf{v}_j, \quad \lambda_1 \geq \lambda_2 \geq ... \geq \lambda_{2k}
\end{equation}

La varianza explicada por los primeros $p$ componentes:

\begin{equation}
\text{VE}_p = \frac{\sum_{j=1}^{p}\lambda_j}{\sum_{j=1}^{2k}\lambda_j} \times 100\%
\end{equation}

% ==============================================================================
\section{Transformaciones Afines}
% ==============================================================================

\subsection{Definición de Transformación Afín}

Una transformación afín en 2D mapea puntos mediante:

\begin{equation}
\begin{pmatrix} x' \\ y' \end{pmatrix} =
\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix} +
\begin{pmatrix} t_x \\ t_y \end{pmatrix}
\end{equation}

O en coordenadas homogéneas:

\begin{equation}
\begin{pmatrix} x' \\ y' \\ 1 \end{pmatrix} =
\begin{pmatrix} a_{11} & a_{12} & t_x \\ a_{21} & a_{22} & t_y \\ 0 & 0 & 1 \end{pmatrix}
\begin{pmatrix} x \\ y \\ 1 \end{pmatrix}
\end{equation}

\subsection{Propiedades de Transformaciones Afines}

\begin{enumerate}
    \item \textbf{Preservan colinealidad}: Puntos colineales permanecen colineales
    \item \textbf{Preservan paralelismo}: Líneas paralelas permanecen paralelas
    \item \textbf{Preservan razones de distancias}: En líneas paralelas a la dirección
    de transformación
    \item \textbf{Grados de libertad}: 6 parámetros en 2D
\end{enumerate}

\subsection{Cálculo de Transformación Afín entre Triángulos}

Dados tres puntos fuente $\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\}$ y sus
correspondientes destinos $\{(x'_1, y'_1), (x'_2, y'_2), (x'_3, y'_3)\}$:

\begin{equation}
\begin{pmatrix}
x'_1 & y'_1 \\
x'_2 & y'_2 \\
x'_3 & y'_3
\end{pmatrix} =
\begin{pmatrix}
x_1 & y_1 & 1 \\
x_2 & y_2 & 1 \\
x_3 & y_3 & 1
\end{pmatrix}
\begin{pmatrix}
a_{11} & a_{21} \\
a_{12} & a_{22} \\
t_x & t_y
\end{pmatrix}
\end{equation}

Sea $\mathbf{S}$ la matriz fuente y $\mathbf{D}$ la matriz destino:

\begin{equation}
\mathbf{A} = (\mathbf{S}^T\mathbf{S})^{-1}\mathbf{S}^T\mathbf{D}
\end{equation}

Para un triángulo no degenerado (puntos no colineales), la solución es única.

% ==============================================================================
\section{Coordenadas Baricéntricas}
% ==============================================================================

\subsection{Definición}

Para un triángulo con vértices $\mathbf{v}_1$, $\mathbf{v}_2$, $\mathbf{v}_3$,
cualquier punto $\mathbf{p}$ en el plano puede expresarse como:

\begin{equation}
\mathbf{p} = \lambda_1\mathbf{v}_1 + \lambda_2\mathbf{v}_2 + \lambda_3\mathbf{v}_3
\end{equation}

donde $\lambda_1 + \lambda_2 + \lambda_3 = 1$.

\subsection{Cálculo de Coordenadas Baricéntricas}

\begin{teorema}[Coordenadas baricéntricas mediante áreas]
Las coordenadas baricéntricas de un punto $\mathbf{p}$ respecto al triángulo
$(\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3)$ son:

\begin{equation}
\lambda_i = \frac{A_i}{A}
\end{equation}

donde $A$ es el área del triángulo completo y $A_i$ es el área del sub-triángulo
opuesto al vértice $\mathbf{v}_i$.
\end{teorema}

\begin{proof}
El área de un triángulo con vértices $\mathbf{a}, \mathbf{b}, \mathbf{c}$ es:

\begin{equation}
A(\mathbf{a}, \mathbf{b}, \mathbf{c}) = \frac{1}{2}|(\mathbf{b} - \mathbf{a}) \times (\mathbf{c} - \mathbf{a})|
\end{equation}

En 2D, el producto cruz da un escalar (componente z):

\begin{equation}
A = \frac{1}{2}|(b_x - a_x)(c_y - a_y) - (c_x - a_x)(b_y - a_y)|
\end{equation}

Para $\lambda_1$:
\begin{equation}
\lambda_1 = \frac{A(\mathbf{p}, \mathbf{v}_2, \mathbf{v}_3)}{A(\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3)}
\end{equation}

Y análogamente para $\lambda_2$ y $\lambda_3$.
\end{proof}

\subsection{Forma Matricial}

Para cálculo eficiente, las coordenadas baricéntricas se obtienen resolviendo:

\begin{equation}
\begin{pmatrix}
x_1 - x_3 & x_2 - x_3 \\
y_1 - y_3 & y_2 - y_3
\end{pmatrix}
\begin{pmatrix}
\lambda_1 \\
\lambda_2
\end{pmatrix} =
\begin{pmatrix}
p_x - x_3 \\
p_y - y_3
\end{pmatrix}
\end{equation}

con $\lambda_3 = 1 - \lambda_1 - \lambda_2$.

Sea $\mathbf{T} = \begin{pmatrix} x_1 - x_3 & x_2 - x_3 \\ y_1 - y_3 & y_2 - y_3 \end{pmatrix}$:

\begin{equation}
\begin{pmatrix} \lambda_1 \\ \lambda_2 \end{pmatrix} = \mathbf{T}^{-1}
\begin{pmatrix} p_x - x_3 \\ p_y - y_3 \end{pmatrix}
\end{equation}

donde:

\begin{equation}
\mathbf{T}^{-1} = \frac{1}{\det(\mathbf{T})}
\begin{pmatrix}
y_2 - y_3 & -(x_2 - x_3) \\
-(y_1 - y_3) & x_1 - x_3
\end{pmatrix}
\end{equation}

\subsection{Propiedades de Pertenencia}

Un punto $\mathbf{p}$ está dentro del triángulo si y solo si:

\begin{equation}
0 \leq \lambda_1 \leq 1, \quad 0 \leq \lambda_2 \leq 1, \quad 0 \leq \lambda_3 \leq 1
\end{equation}

Esta propiedad permite determinar eficientemente a qué triángulo pertenece
cada píxel durante el warping.

% ==============================================================================
\section{Warping Piecewise Affine}
% ==============================================================================

\subsection{Formulación del Problema}

Dada una imagen fuente $I_s$ con landmarks $\mathbf{L}_s = \{(x_i^s, y_i^s)\}_{i=1}^k$
y landmarks destino $\mathbf{L}_d = \{(x_i^d, y_i^d)\}_{i=1}^k$, el objetivo es
crear una imagen destino $I_d$ donde cada píxel en posición $(x, y)$ toma su
valor de la posición correspondiente en $I_s$.

\subsection{Triangulación de Delaunay}

La triangulación de Delaunay $\mathcal{T}$ sobre los landmarks satisface la
propiedad del círculo vacío:

\begin{definicion}[Propiedad de Delaunay]
Para cada triángulo en $\mathcal{T}$, su circuncírculo no contiene ningún
otro punto del conjunto de landmarks.
\end{definicion}

\begin{teorema}[Optimalidad de Delaunay]
La triangulación de Delaunay maximiza el ángulo mínimo entre todas las
triangulaciones posibles, minimizando triángulos delgados.
\end{teorema}

\subsection{Algoritmo de Warping Completo}

\begin{algorithm}
\caption{Warping Piecewise Affine}
\label{alg:piecewise_warp}
\begin{algorithmic}[1]
\REQUIRE $I_s$: Imagen fuente
\REQUIRE $\mathbf{L}_s$: Landmarks fuente
\REQUIRE $\mathbf{L}_d$: Landmarks destino
\ENSURE $I_d$: Imagen warpeada

\STATE $\mathcal{T} \leftarrow \text{DelaunayTriangulation}(\mathbf{L}_d)$
\STATE $I_d \leftarrow \text{zeros}(\text{size}(I_s))$

\FOR{cada triángulo $T_j = (v_1, v_2, v_3) \in \mathcal{T}$}
    \STATE $T_j^d \leftarrow (\mathbf{L}_d[v_1], \mathbf{L}_d[v_2], \mathbf{L}_d[v_3])$ \COMMENT{Triángulo destino}
    \STATE $T_j^s \leftarrow (\mathbf{L}_s[v_1], \mathbf{L}_s[v_2], \mathbf{L}_s[v_3])$ \COMMENT{Triángulo fuente}
    \STATE $\mathbf{A}_j \leftarrow \text{ComputeAffine}(T_j^d, T_j^s)$ \COMMENT{Transformación afín}

    \FOR{cada píxel $(x, y)$ en el bounding box de $T_j^d$}
        \STATE $(\lambda_1, \lambda_2, \lambda_3) \leftarrow \text{Barycentric}(x, y, T_j^d)$
        \IF{$\lambda_1 \geq 0$ \AND $\lambda_2 \geq 0$ \AND $\lambda_3 \geq 0$}
            \STATE $(x_s, y_s) \leftarrow \mathbf{A}_j \cdot (x, y, 1)^T$
            \STATE $I_d[y, x] \leftarrow \text{BilinearInterpolate}(I_s, x_s, y_s)$
        \ENDIF
    \ENDFOR
\ENDFOR

\RETURN $I_d$
\end{algorithmic}
\end{algorithm}

\subsection{Interpolación Bilineal}

Para valores sub-píxel $(x_s, y_s)$:

\begin{equation}
I(x_s, y_s) = (1-\alpha)(1-\beta)I[\lfloor y_s \rfloor, \lfloor x_s \rfloor] +
              \alpha(1-\beta)I[\lfloor y_s \rfloor, \lceil x_s \rceil] +
\end{equation}
\begin{equation}
              (1-\alpha)\beta I[\lceil y_s \rceil, \lfloor x_s \rfloor] +
              \alpha\beta I[\lceil y_s \rceil, \lceil x_s \rceil]
\end{equation}

donde $\alpha = x_s - \lfloor x_s \rfloor$ y $\beta = y_s - \lfloor y_s \rfloor$.

% ==============================================================================
\section{Funciones de Pérdida}
% ==============================================================================

\subsection{Mean Squared Error (MSE) para Regresión}

Para predicción de landmarks:

\begin{equation}
\mathcal{L}_{\text{MSE}} = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{k}\sum_{j=1}^{k}\|\hat{\mathbf{l}}_{ij} - \mathbf{l}_{ij}\|_2^2
\end{equation}

donde $\hat{\mathbf{l}}_{ij} \in \mathbb{R}^2$ es el landmark $j$ predicho para
la imagen $i$ y $\mathbf{l}_{ij}$ es el ground truth.

\subsection{Cross-Entropy para Clasificación}

Para clasificación multi-clase:

\begin{equation}
\mathcal{L}_{\text{CE}} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{ic}\log(\hat{y}_{ic})
\end{equation}

donde $y_{ic} \in \{0, 1\}$ es el indicador de clase y $\hat{y}_{ic}$ es la
probabilidad predicha.

\subsection{Weighted Cross-Entropy}

Para manejo de clases desbalanceadas:

\begin{equation}
\mathcal{L}_{\text{WCE}} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}w_c \cdot y_{ic}\log(\hat{y}_{ic})
\end{equation}

donde $w_c = \frac{n}{C \cdot n_c}$ con $n_c$ el número de muestras de clase $c$.

\subsection{Derivada de Cross-Entropy con Softmax}

Para la capa final con softmax:

\begin{equation}
\hat{y}_c = \frac{e^{z_c}}{\sum_{j=1}^{C}e^{z_j}}
\end{equation}

La derivada de la pérdida respecto a los logits $z_c$:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial z_c} = \hat{y}_c - y_c
\end{equation}

Esta forma simplificada es numéricamente estable y eficiente para backpropagation.

% ==============================================================================
\section{Estadísticos de Evaluación}
% ==============================================================================

\subsection{Métricas de Regresión}

\subsubsection{Mean Absolute Error (MAE)}

\begin{equation}
\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{k}\sum_{j=1}^{k}\|\hat{\mathbf{l}}_{ij} - \mathbf{l}_{ij}\|_1
\end{equation}

En el caso 2D:
\begin{equation}
\text{MAE} = \frac{1}{nk}\sum_{i=1}^{n}\sum_{j=1}^{k}(|\hat{x}_{ij} - x_{ij}| + |\hat{y}_{ij} - y_{ij}|)
\end{equation}

\subsubsection{Distancia Euclidiana Promedio}

\begin{equation}
\text{MED} = \frac{1}{nk}\sum_{i=1}^{n}\sum_{j=1}^{k}\sqrt{(\hat{x}_{ij} - x_{ij})^2 + (\hat{y}_{ij} - y_{ij})^2}
\end{equation}

\subsection{Métricas de Clasificación}

\subsubsection{Precision, Recall, F1}

Para clase $c$:

\begin{align}
\text{Precision}_c &= \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c} \\
\text{Recall}_c &= \frac{\text{TP}_c}{\text{TP}_c + \text{FN}_c} \\
\text{F1}_c &= \frac{2 \cdot \text{Precision}_c \cdot \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}
\end{align}

\subsubsection{Macro-Average}

\begin{equation}
\text{Macro-F1} = \frac{1}{C}\sum_{c=1}^{C}\text{F1}_c
\end{equation}

\subsubsection{AUC-ROC}

Para clasificación binaria, el área bajo la curva ROC:

\begin{equation}
\text{AUC} = \int_0^1 \text{TPR}(\text{FPR}^{-1}(t)) dt
\end{equation}

Equivalentemente, la probabilidad de que un ejemplo positivo aleatorio tenga
mayor score que un negativo aleatorio:

\begin{equation}
\text{AUC} = P(\hat{y}_{+} > \hat{y}_{-})
\end{equation}

\subsection{Test de McNemar}

Para comparar dos clasificadores $A$ y $B$:

\begin{equation}
\chi^2 = \frac{(n_{01} - n_{10})^2}{n_{01} + n_{10}}
\end{equation}

donde:
\begin{itemize}
    \item $n_{01}$: casos donde A falla y B acierta
    \item $n_{10}$: casos donde A acierta y B falla
\end{itemize}

Bajo $H_0$ (sin diferencia), $\chi^2 \sim \chi^2_1$.

\subsection{ANOVA para Análisis de Robustez}

Modelo de efectos fijos:

\begin{equation}
y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}
\end{equation}

donde:
\begin{itemize}
    \item $\alpha_i$: efecto del tipo de modelo (original/warped)
    \item $\beta_j$: efecto del tipo de perturbación
    \item $(\alpha\beta)_{ij}$: interacción
    \item $\epsilon_{ijk} \sim N(0, \sigma^2)$: error
\end{itemize}

Estadístico F:
\begin{equation}
F = \frac{\text{MS}_{\text{between}}}{\text{MS}_{\text{within}}} = \frac{SS_{\text{between}} / (k-1)}{SS_{\text{within}} / (N-k)}
\end{equation}

% ==============================================================================
\section{Conclusiones}
% ==============================================================================

Este apéndice ha presentado las derivaciones matemáticas completas de los
algoritmos fundamentales utilizados en el proyecto:

\begin{enumerate}
    \item La solución del problema Procrustes mediante SVD
    \item La convergencia del algoritmo GPA
    \item Las transformaciones afines para warping
    \item Las coordenadas baricéntricas para interpolación
    \item Las funciones de pérdida y sus gradientes
    \item Los estadísticos para evaluación
\end{enumerate}

Estas derivaciones proporcionan el fundamento teórico riguroso que soporta
los resultados experimentales presentados en los documentos principales.

\end{document}
