\section{Marco Teórico}
\label{sec:marco-teorico}

% JUSTIFICACIÓN DE ESTA SECCIÓN:
% - Proporciona fundamentos físicos y matemáticos
% - Establece base para entender la metodología
% - Énfasis ingenieril con fórmulas relevantes

\subsection{Radiografías de Tórax}
\label{subsec:radiografias}

La radiografía de tórax se fundamenta en la atenuación diferencial de rayos X al atravesar tejidos de distinta densidad \citep{bushberg2011essential}. La intensidad transmitida $I(x)$ sigue la ley de Beer-Lambert:

\begin{equation}\label{eq:beer-lambert}
I(x) = I_0 \exp\left(-\int_0^x \mu(s) \, ds\right)
\end{equation}

donde $I_0$ es la intensidad incidente, $\mu(s)$ el coeficiente de atenuación lineal (dependiente del tejido), y $x$ la distancia recorrida.

Los coeficientes típicos de atenuación son:
\begin{itemize}
    \item Aire alveolar: $\mu \approx 0.0001$ cm$^{-1}$
    \item Tejidos blandos: $\mu \approx 0.20$ cm$^{-1}$
    \item Hueso cortical: $\mu \approx 0.50$ cm$^{-1}$
\end{itemize}

\subsubsection{Landmarks Anatómicos}
\label{subsubsec:landmarks}

Se definen 15 puntos de referencia que caracterizan la geometría torácica:

\begin{itemize}
    \item \textbf{Eje central} (L1, L2): Define la línea media vertical
    \item \textbf{Puntos centrales} (L9, L10, L11): Dividen el eje en cuartos
    \item \textbf{Pares bilaterales}: (L3-L4), (L5-L6), (L7-L8), (L12-L13), (L14-L15)
\end{itemize}

Estos landmarks presentan propiedades geométricas verificables: los puntos centrales se ubican exactamente en $t \in \{0.25, 0.50, 0.75\}$ a lo largo del eje, con desviación menor a 1.5 píxeles.

\subsection{Redes Neuronales Convolucionales}
\label{subsec:cnn}

Las CNNs \citep{lecun1998gradient} realizan extracción jerárquica de características mediante la operación de convolución discreta:

\begin{equation}\label{eq:convolucion}
Y[i,j] = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} X[i+m, j+n] \cdot W[m,n] + b
\end{equation}

donde $X$ es la entrada, $W$ el kernel de convolución, y $b$ el sesgo.

\subsubsection{Arquitectura ResNet}
\label{subsubsec:resnet}

Las redes residuales \citep{he2016deep} introducen conexiones de salto (skip connections) que permiten entrenar redes más profundas:

\begin{equation}\label{eq:residual}
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
\end{equation}

donde $\mathcal{F}$ representa las capas residuales y $\mathbf{x}$ la conexión directa.

ResNet-18, con 11.7 millones de parámetros, ofrece un balance óptimo entre capacidad representacional y eficiencia computacional para tareas de regresión de coordenadas.

\subsubsection{Transfer Learning}
\label{subsubsec:transfer}

El preentrenamiento en ImageNet (1.2 millones de imágenes, 1000 clases) proporciona:
\begin{itemize}
    \item Filtros de bajo nivel (bordes, texturas) altamente transferibles
    \item Inicialización superior a pesos aleatorios
    \item Convergencia más rápida con menos datos médicos
\end{itemize}

Para tareas con menos de 10,000 imágenes etiquetadas, el transfer learning mejora consistentemente el rendimiento comparado con entrenamiento desde cero \citep{yosinski2014transferable}.

\subsection{Funciones de Pérdida para Regresión de Landmarks}
\label{subsec:loss-functions}

La regresión de coordenadas de landmarks presenta desafíos específicos que requieren funciones de pérdida especializadas.

\subsubsection{Wing Loss}
\label{subsubsec:wing-loss}

Para la localización precisa de landmarks, la función Wing Loss \citep{feng2018wing} ofrece mejor convergencia que el error cuadrático medio (MSE) al balancear la sensibilidad a errores pequeños y grandes:

\begin{equation}\label{eq:wing-loss}
\mathcal{L}_{\text{wing}}(y, \hat{y}) =
\begin{cases}
w \ln\left(1 + \frac{|y-\hat{y}|}{\epsilon}\right) & \text{si } |y-\hat{y}| < w \\
|y-\hat{y}| - C & \text{en otro caso}
\end{cases}
\end{equation}

donde $w$ controla el rango no lineal (típicamente $w=10$), $\epsilon$ previene división por cero ($\epsilon=2$), y $C = w - w\ln(1 + w/\epsilon)$ asegura continuidad en $|y-\hat{y}| = w$.

Esta formulación penaliza más fuertemente los errores pequeños (región logarítmica) mientras mantiene comportamiento lineal para outliers, resultando en predicciones más precisas cerca del ground truth.

\subsection{Normalización Geométrica}
\label{subsec:normalizacion}

\subsubsection{Análisis de Procrustes Generalizado}
\label{subsubsec:procrustes}

La alineación de formas utiliza el Análisis de Procrustes Generalizado (GPA) \citep{gower1975generalized,dryden2016statistical} para encontrar la transformación óptima entre conjuntos de landmarks. Dadas dos configuraciones de puntos $X$ e $Y$, la rotación óptima se obtiene mediante:

\begin{equation}\label{eq:procrustes}
R^* = V U^T \quad \text{donde} \quad \text{SVD}(X^T Y) = U \Sigma V^T
\end{equation}

La transformación afín completa incluye traslación, escala y rotación, minimizando la distancia de Procrustes entre la configuración observada y una forma de referencia canónica.

\subsubsection{Warping Afín por Partes}
\label{subsubsec:warping}

El warping afín por partes divide la imagen en regiones triangulares (triangulación de Delaunay) y aplica transformaciones afines locales $T_i(\mathbf{x}) = A_i\mathbf{x} + \mathbf{b}_i$ a cada triángulo. Cada matriz $A_i \in \mathbb{R}^{2\times 2}$ se determina por las correspondencias de los tres vértices del triángulo entre la configuración original y la normalizada.

El \textit{fill rate} (porcentaje de cobertura) cuantifica la fracción del área de salida con información válida después del warping:
\begin{equation}\label{eq:fill-rate}
\text{fill\_rate} = 1 - \frac{\text{píxeles\_negros}}{\text{total\_píxeles}}
\end{equation}

Valores típicos son 47\% (warping conservativo, solo región central) y 96\% (cobertura extendida con interpolación en bordes).
