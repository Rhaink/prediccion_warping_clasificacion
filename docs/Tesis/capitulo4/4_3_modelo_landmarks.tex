% =============================================================================
% CAPÍTULO 4: METODOLOGÍA
% Sección 4.3: Modelo de Predicción de Landmarks
% =============================================================================

\section{Modelo de Predicción de Puntos de Referencia}
\label{sec:modelo_landmarks}

El modelo de predicción de puntos de referencia constituye el primer componente del sistema propuesto y tiene como objetivo localizar los 15 puntos de referencia anatómicos que definen el contorno pulmonar en cada radiografía. Esta sección describe la arquitectura del modelo, la función de pérdida utilizada y la estrategia de entrenamiento implementada.

\subsection{Arquitectura del Modelo}
\label{subsec:arquitectura_modelo}

El modelo propuesto se basa en una arquitectura de red neuronal convolucional con tres componentes principales: un módulo de extracción de características, un módulo de atención y un módulo de regresión. La Figura \ref{fig:arquitectura_modelo} presenta el diagrama de la arquitectura completa.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/F4.5_arquitectura_modelo.png}
    \caption{Arquitectura del modelo de predicción de puntos de referencia. ResNet-18 extrae características de alto nivel, el módulo Coordinate Attention incorpora información posicional, y la cabeza de regresión predice las 30 coordenadas (15 puntos de referencia $\times$ 2 coordenadas).}
    \label{fig:arquitectura_modelo}
\end{figure}

\subsubsection{ResNet-18}

Como extractor de características (denominado \textit{extractor de caracter\'isticas} en la literatura de aprendizaje profundo) se utiliza ResNet-18 \cite{he2016deep}, una red residual de 18 capas preentrenada en el conjunto de datos ImageNet \cite{deng2009imagenet}. La arquitectura ResNet introdujo las conexiones residuales (\textit{skip connections}) que permiten entrenar redes más profundas al mitigar el problema de desvanecimiento de gradiente.

La elección de ResNet-18 sobre arquitecturas más profundas (ResNet-34, ResNet-50) se fundamenta en las siguientes consideraciones:

\begin{enumerate}
    \item \textbf{Tamaño del conjunto de datos:} Con 957 imágenes anotadas, un modelo más pequeño reduce el riesgo de sobreajuste.
    \item \textbf{Eficiencia computacional:} ResNet-18 permite iteraciones de entrenamiento más rápidas durante la experimentación.
    \item \textbf{Suficiente capacidad:} La tarea de localización de 15 puntos de referencia no requiere la capacidad de representación de arquitecturas más profundas.
    \item \textbf{Aprendizaje por transferencia efectivo:} Los pesos preentrenados en ImageNet proporcionan características genéricas útiles para imágenes médicas.
\end{enumerate}

El extractor de características procesa imágenes de entrada de dimensiones $224 \times 224 \times 3$ y produce un mapa de características de dimensiones $7 \times 7 \times 512$. La Tabla \ref{tab:extractor_caracteristicas_arquitectura} detalla las capas del extractor de características utilizadas.

\begin{table}[htbp]
    \centering
    \caption{Configuración del extractor de características ResNet-18. Se utilizan todas las capas convolucionales, removiendo únicamente la capa fully connected original.}
    \label{tab:extractor_caracteristicas_arquitectura}
    \begin{tabular}{llcc}
        \toprule
        \textbf{Capa} & \textbf{Descripción} & \textbf{Salida} & \textbf{Parámetros} \\
        \midrule
        conv1 & Conv $7 \times 7$, stride 2 & $112 \times 112 \times 64$ & 9,408 \\
        bn1 + relu & BatchNorm + ReLU & $112 \times 112 \times 64$ & 128 \\
        maxpool & MaxPool $3 \times 3$, stride 2 & $56 \times 56 \times 64$ & 0 \\
        layer1 & 2 bloques residuales & $56 \times 56 \times 64$ & 147,968 \\
        layer2 & 2 bloques residuales & $28 \times 28 \times 128$ & 525,568 \\
        layer3 & 2 bloques residuales & $14 \times 14 \times 256$ & 2,099,712 \\
        layer4 & 2 bloques residuales & $7 \times 7 \times 512$ & 8,393,728 \\
        \midrule
        \multicolumn{2}{l}{\textbf{Total extractor de caracter\'isticas}} & --- & \textbf{11,176,512} \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Módulo Coordinate Attention}

\paragraph{Motivación y propósito.}
Cuando una red neuronal procesa una imagen, extrae información sobre texturas, formas y patrones, pero puede perder la noción de \textit{dónde} se encuentran estos elementos dentro de la imagen. Para una tarea de localización de puntos de referencia, saber \textit{dónde} mirar es tan importante como saber \textit{qué} buscar.

El módulo Coordinate Attention \cite{hou2021coordinate} aborda este problema permitiendo que el modelo ``aprenda a enfocar su atención'' en las regiones más relevantes de la imagen, manteniendo información sobre la posición horizontal y vertical de cada elemento. En términos simples, funciona como un mecanismo que le indica al modelo: ``presta más atención a esta fila de la imagen'' y ``presta más atención a esta columna''.

\paragraph{Diferencia con otros mecanismos de atención.}
Existen otros mecanismos de atención, como SE-Net \cite{hu2018squeeze}, que resumen toda la información espacial de la imagen en un único valor promedio. Si bien esto permite identificar \textit{qué} características son importantes, pierde la información de \textit{dónde} se encuentran. Para la detección de puntos de referencia, donde la posición exacta es el objetivo principal, esta pérdida de información espacial resulta problemática.

Coordinate Attention resuelve esta limitación procesando la información de posición de manera separada: genera un ``mapa de importancia'' para las filas de la imagen y otro para las columnas. De esta forma, el modelo puede aprender que ciertas regiones horizontales (por ejemplo, donde típicamente se encuentra el contorno superior pulmonar) y ciertas regiones verticales (donde se ubica el contorno lateral) merecen mayor atención.

\paragraph{Funcionamiento del módulo.}
El proceso se realiza en cuatro pasos:

\textbf{Paso 1: Resumen por filas y columnas.} Primero, el módulo calcula un resumen de la información para cada fila y cada columna de la imagen. Para cada fila $h$, se promedian todos los valores a lo largo de esa fila:

\begin{equation}
    z_c^h(h) = \frac{1}{W} \sum_{w=1}^{W} x_c(h, w)
\end{equation}

De manera análoga, para cada columna $w$:

\begin{equation}
    z_c^w(w) = \frac{1}{H} \sum_{h=1}^{H} x_c(h, w)
\end{equation}

\noindent donde $x_c(h, w)$ representa el valor en la posición $(h, w)$ del canal $c$, $H$ es la altura y $W$ el ancho de la imagen de características.

\textbf{Paso 2: Combinación y procesamiento.} Los resúmenes de filas y columnas se combinan y procesan conjuntamente. Esta combinación permite que el módulo analice la relación entre la información horizontal y vertical para identificar qué regiones son relevantes. Para mantener la eficiencia computacional, la información se comprime temporalmente a una representación más compacta (en este caso, de 512 a 16 valores intermedios) antes de generar los pesos de atención.

\textbf{Paso 3: Generación de pesos de atención.} A partir de la información combinada, se generan dos conjuntos de pesos: uno que indica la importancia de cada fila ($\mathbf{a}^h$) y otro para cada columna ($\mathbf{a}^w$). Estos pesos se calculan de forma que cada valor quede en el rango entre 0 y 1, donde 0 significa ``ignorar completamente'' y 1 significa ``prestar máxima atención''.

\textbf{Paso 4: Aplicación de la atención.} Finalmente, cada posición de la imagen se pondera según la importancia de su fila y su columna:

\begin{equation}
    y_c(h, w) = x_c(h, w) \cdot a_c^h(h) \cdot a_c^w(w)
\end{equation}

De esta manera, las regiones donde tanto la fila como la columna tienen alta importancia reciben mayor peso, mientras que las regiones irrelevantes se atenúan.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/coord_attention_v10_mechanism_real.png}
    \caption{Mecanismo del módulo Coordinate Attention y ejemplo de mapa de atención espacial sobre una radiografía. El módulo estima pesos por fila ($a^h$) y por columna ($a^w$), y los aplica para reponderar el mapa de características.}
    \label{fig:coord_attention_mechanism}
\end{figure}

\paragraph{Parámetros del módulo.}
La Tabla \ref{tab:coord_attention_params} presenta los componentes y parámetros del módulo Coordinate Attention.

\begin{table}[htbp]
    \centering
    \caption{Parámetros del módulo Coordinate Attention para $C=512$ canales de entrada.}
    \label{tab:coord_attention_params}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Componente} & \textbf{Configuración} & \textbf{Parámetros} \\
        \midrule
        pool\_h & AdaptiveAvgPool2d(None, 1) & 0 \\
        pool\_w & AdaptiveAvgPool2d(1, None) & 0 \\
        conv1 & Conv2d(512, 16, núcleo=1) & 8,192 \\
        bn1 & BatchNorm2d(16) & 32 \\
        conv\_h & Conv2d(16, 512, núcleo=1) & 8,192 \\
        conv\_w & Conv2d(16, 512, núcleo=1) & 8,192 \\
        \midrule
        \multicolumn{2}{l}{\textbf{Total Coordinate Attention}} & \textbf{24,608} \\
        \bottomrule
    \end{tabular}

    \vspace{0.5em}
\end{table}

\subsubsection{Cabeza de Regresión}

\paragraph{Propósito.}
Mientras que el extractor de características (ResNet-18) procesa la imagen y el módulo de atención identifica las regiones relevantes, la cabeza de regresión tiene la tarea final: convertir toda esta información en las 30 coordenadas numéricas que representan la posición de los 15 puntos de referencia (cada punto de referencia tiene una coordenada $x$ y una coordenada $y$).

\paragraph{Arquitectura.}
La cabeza de regresión procesa la información a través de varias etapas:

\begin{enumerate}
    \item \textbf{Condensación espacial:} Primero, se resume toda la información del mapa de características ($7 \times 7$ posiciones con 512 valores cada una) en un único vector de 512 valores. Este paso, denominado \textit{Global Average Pooling}, calcula el promedio de cada característica a través de todas las posiciones espaciales.

    \item \textbf{Capas de transformación:} El vector resultante pasa por dos capas que transforman progresivamente la información. La primera capa mantiene 512 valores, y la segunda expande a 768 valores. Esta expansión permite que el modelo capture relaciones más complejas entre las características antes de producir la salida final.

    \item \textbf{Capa de salida:} Finalmente, una última capa reduce los 768 valores a exactamente 30 números, que corresponden a las coordenadas de los 15 puntos de referencia. Estos valores se restringen al rango $[0, 1]$ mediante una función sigmoide, representando posiciones normalizadas respecto al tamaño de la imagen.
\end{enumerate}

\paragraph{Técnicas de estabilización.}
Para que el entrenamiento sea estable y el modelo generalice bien, se incorporan dos técnicas:

\begin{itemize}
    \item \textbf{Normalización por grupos} (\textit{Group Normalization}) \cite{wu2018group}: Estandariza los valores intermedios para evitar que crezcan o decrezcan de manera descontrolada durante el entrenamiento. A diferencia de otras técnicas de normalización, esta funciona de manera consistente independientemente del número de imágenes procesadas simultáneamente.

    \item \textbf{Dropout}: Durante el entrenamiento, se ``apagan'' aleatoriamente algunos valores (30\% en la primera capa, 15\% en la segunda). Esto obliga al modelo a no depender excesivamente de ninguna característica particular, mejorando su capacidad de generalización.
\end{itemize}

Para obtener las coordenadas finales en píxeles, las predicciones normalizadas se multiplican por el tamaño de la imagen (224 píxeles). La Tabla \ref{tab:head_arquitectura} detalla la arquitectura completa de la cabeza de regresión.

\begin{table}[htbp]
    \centering
    \caption{Arquitectura de la cabeza de regresión con normalización por grupos.}
    \label{tab:head_arquitectura}
    \begin{tabular}{llcc}
        \toprule
        \textbf{Capa} & \textbf{Operación} & \textbf{Salida} & \textbf{Parámetros} \\
        \midrule
        avgpool & AdaptiveAvgPool2d(1,1) & 512 & 0 \\
        flatten & Flatten & 512 & 0 \\
        fc1 & Linear(512, 512) & 512 & 262,656 \\
        gn1 & GroupNorm(32, 512) & 512 & 1,024 \\
        relu1 & ReLU & 512 & 0 \\
        dropout1 & Dropout(p=0.3) & 512 & 0 \\
        fc2 & Linear(512, 768) & 768 & 394,752 \\
        gn2 & GroupNorm(48, 768) & 768 & 1,536 \\
        relu2 & ReLU & 768 & 0 \\
        dropout2 & Dropout(p=0.15) & 768 & 0 \\
        fc3 & Linear(768, 30) & 30 & 23,070 \\
        sigmoid & Sigmoid & 30 & 0 \\
        \midrule
        \multicolumn{2}{l}{\textbf{Total cabeza}} & --- & \textbf{683,038} \\
        \bottomrule
    \end{tabular}
\end{table}

El número total de parámetros del modelo completo es aproximadamente 11.9 millones, de los cuales 11.2 millones corresponden al extractor de características preentrenado.

\subsection{Función de Pérdida}
\label{subsec:funcion_perdida}

\paragraph{Concepto de función de pérdida.}
Durante el entrenamiento, el modelo necesita una forma de medir qué tan lejos están sus predicciones de las posiciones reales de los puntos de referencia. Esta medida se denomina \textit{función de pérdida}: un valor numérico que indica qué tan ``equivocado'' está el modelo. El objetivo del entrenamiento es ajustar los parámetros del modelo para minimizar este valor.

\paragraph{Limitaciones de las funciones tradicionales.}
Las funciones de pérdida más comunes son el error absoluto (L1) y el error cuadrático (L2). Sin embargo, ambas presentan limitaciones para la localización precisa de puntos de referencia:

\begin{itemize}
    \item \textbf{Error cuadrático (L2):} Penaliza fuertemente los errores grandes, lo cual es útil al inicio del entrenamiento cuando las predicciones están muy alejadas. Sin embargo, cuando las predicciones ya están cerca del objetivo, la penalización se vuelve muy pequeña y el modelo pierde incentivo para seguir mejorando.

    \item \textbf{Error absoluto (L1):} Trata todos los errores de manera uniforme, sin importar su magnitud. Esto significa que un error de 1 píxel recibe la misma ``urgencia'' de corrección que un error de 20 píxeles, lo cual no es óptimo.
\end{itemize}

\paragraph{Wing Loss: comportamiento adaptativo.}
Wing Loss \cite{feng2018wing} fue diseñada específicamente para localización de puntos de referencia, combinando las ventajas de ambos enfoques:

\begin{itemize}
    \item \textbf{Para errores pequeños} (menores a $\omega$ píxeles): Aplica una penalización que crece rápidamente, incentivando al modelo a refinar las predicciones que ya están cerca del objetivo. Esto es crucial para lograr precisión subpixel.

    \item \textbf{Para errores grandes} (mayores a $\omega$ píxeles): Se comporta de manera similar a L1, proporcionando correcciones estables sin penalizaciones extremas que podrían desestabilizar el entrenamiento.
\end{itemize}

La formulación matemática es:

\begin{equation}
    \text{wing}(x) =
    \begin{cases}
        \omega \ln\left(1 + \frac{|x|}{\epsilon}\right) & \text{si } |x| < \omega \\
        |x| - C & \text{de otro modo}
    \end{cases}
    \label{eq:wing_loss}
\end{equation}

\noindent donde $x$ es la diferencia entre la coordenada predicha y la real, $\omega$ define el umbral entre los dos comportamientos, $\epsilon$ controla la sensibilidad en la región de errores pequeños, y $C$ es una constante que asegura continuidad entre ambas regiones.

\paragraph{Parámetros utilizados.}
En este trabajo se utilizan $\omega = 10$ píxeles y $\epsilon = 2$ píxeles. Esto significa que errores menores a 10 píxeles reciben el tratamiento de ``refinamiento fino'', mientras que errores mayores reciben correcciones estables. Dado que las coordenadas se normalizan al rango $[0, 1]$ durante el entrenamiento, estos valores se escalan proporcionalmente.

La Figura \ref{fig:wing_loss} ilustra el comportamiento de Wing Loss comparado con L1 y L2.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/F4.6_wing_loss_grafica.png}
    \caption{Comparación de Wing Loss con pérdidas L1 y L2. Wing Loss proporciona mayor penalización para errores pequeños (región logarítmica) incentivando el refinamiento fino, mientras mantiene estabilidad para errores grandes (región lineal).}
    \label{fig:wing_loss}
\end{figure}

La pérdida total del modelo se calcula promediando Wing Loss sobre las 30 coordenadas (15 puntos de referencia $\times$ 2 coordenadas):

\begin{equation}
    \mathcal{L}_{\text{total}} = \frac{1}{30} \sum_{i=1}^{30} \text{wing}(\hat{c}_i - c_i)
    \label{eq:total_loss}
\end{equation}

\noindent donde $\hat{c}_i$ representa la coordenada predicha y $c_i$ la coordenada real.

\subsection{Estrategia de Entrenamiento}
\label{subsec:estrategia_entrenamiento}

\paragraph{Aprendizaje por transferencia.}
Entrenar una red neuronal desde cero requiere grandes cantidades de datos para que el modelo aprenda a reconocer patrones visuales básicos (bordes, texturas, formas). Sin embargo, cuando se dispone de un conjunto de datos limitado (como las 957 radiografías anotadas de este trabajo) existe el riesgo de que el modelo memorice los ejemplos de entrenamiento en lugar de aprender patrones generalizables.

El \textit{aprendizaje por transferencia} \cite{yosinski2014transferable} aborda este problema reutilizando un modelo previamente entrenado en una tarea relacionada. En este caso, se utiliza ResNet-18 preentrenado en ImageNet, un conjunto de más de un millón de imágenes naturales. Aunque las radiografías de tórax son visualmente diferentes a las fotografías de ImageNet, las capas iniciales de la red aprenden detectores de características básicas (bordes, gradientes, texturas) que son útiles para cualquier tarea de visión por computadora.

El entrenamiento se divide en dos fases para aprovechar este conocimiento previo de manera efectiva.

\subsubsection{Fase 1: Entrenamiento de la Cabeza}

En la primera fase, se ``congelan'' los parámetros del extractor de características (ResNet-18) y del módulo Coordinate Attention, es decir, estos parámetros no se modifican durante el entrenamiento. Únicamente se entrenan los parámetros de la cabeza de regresión.

Esta estrategia tiene dos objetivos:

\begin{enumerate}
    \item \textbf{Preservar el conocimiento previo:} Las capas convolucionales ya saben detectar bordes, texturas y formas. Modificarlas prematuramente podría destruir este conocimiento antes de que la cabeza aprenda a utilizarlo.

    \item \textbf{Establecer una base estable:} La cabeza de regresión comienza con pesos aleatorios, por lo que sus predicciones iniciales son esencialmente ruido. Entrenarla primero permite que aprenda a interpretar las características extraídas por ResNet-18 antes de que estas cambien.
\end{enumerate}

La configuración de la Fase 1 se presenta en la Tabla \ref{tab:fase1_config}.

\begin{table}[htbp]
    \centering
    \caption{Configuración de entrenamiento - Fase 1.}
    \label{tab:fase1_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parámetro} & \textbf{Valor} \\
        \midrule
        Épocas máximas & 15 \\
        Tasa de aprendizaje & $1 \times 10^{-3}$ \\
        Optimizador & Adam ($\beta_1=0.9$, $\beta_2=0.999$) \\
        Tamaño de lote & 16 \\
        Parámetros entrenables & Solo cabeza (683,038) \\
        Parámetros congelados & Backbone + CoordAttn (11,201,120) \\
        Parada temprana & Paciencia = 5 épocas \\
        Métrica de monitoreo & Error de validación (píxeles) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Fase 2: Ajuste Fino Completo}

Una vez que la cabeza de regresión ha aprendido a producir predicciones razonables, se ``descongelan'' todos los parámetros del modelo para realizar un ajuste fino completo. En esta fase, tanto el extractor de características como la cabeza se adaptan específicamente a la tarea de localización de puntos de referencia en radiografías.

\paragraph{Tasas de aprendizaje diferenciadas.}
Un riesgo de modificar el extractor de características es que los ajustes sean tan grandes que destruyan el conocimiento previamente aprendido. Para mitigar este riesgo, se utilizan tasas de aprendizaje diferentes para cada parte del modelo:

\begin{itemize}
    \item \textbf{Extractor + Coordinate Attention:} Tasa de aprendizaje baja ($2 \times 10^{-5}$), permitiendo solo ajustes sutiles que especialicen las características para radiografías sin perder la capacidad general de detección.

    \item \textbf{Cabeza de regresión:} Tasa de aprendizaje 10 veces mayor ($2 \times 10^{-4}$), permitiendo que continúe adaptándose más rápidamente.
\end{itemize}

Esta proporción 10:1 refleja la intuición de que el extractor ya tiene conocimiento valioso que debe preservarse, mientras que la cabeza aún tiene margen para mejorar.

\paragraph{Reducción gradual de la tasa de aprendizaje.}
A medida que el entrenamiento avanza, el modelo se acerca a una solución óptima y los ajustes necesarios son cada vez más pequeños. Para reflejar esto, se utiliza un esquema de \textit{Cosine Annealing} \cite{loshchilov2016sgdr} que reduce la tasa de aprendizaje de manera suave: comienza con el valor inicial y decrece gradualmente hasta un mínimo ($10^{-6}$), siguiendo una curva en forma de coseno. Este decrecimiento suave evita cambios bruscos que podrían desestabilizar el entrenamiento en las etapas finales.

La Tabla \ref{tab:fase2_config} detalla la configuración de la Fase 2.

\begin{table}[htbp]
    \centering
    \caption{Configuración de entrenamiento - Fase 2.}
    \label{tab:fase2_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parámetro} & \textbf{Valor} \\
        \midrule
        Épocas máximas & 100 \\
        Tasa de aprendizaje (extractor de caracter\'isticas + CA) & $2 \times 10^{-5}$ \\
        Tasa de aprendizaje (cabeza) & $2 \times 10^{-4}$ \\
        Optimizador & Adam ($\beta_1=0.9$, $\beta_2=0.999$) \\
        Scheduler & Cosine Annealing ($T=100$, $\eta_{\min}=10^{-6}$) \\
        Tamaño de lote & 8 \\
        Parámetros entrenables & Todos (11,884,158) \\
        Parada temprana & Paciencia = 15 épocas \\
        Métrica de monitoreo & Error de validación (píxeles) \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Aumento de Datos}

Con solo 957 imágenes anotadas, existe el riesgo de que el modelo memorice características específicas de las imágenes de entrenamiento (como la posición exacta de ciertos artefactos o variaciones particulares de iluminación) en lugar de aprender el concepto general de ``contorno pulmonar''. El \textit{aumento de datos} mitiga este riesgo creando versiones modificadas de las imágenes de entrenamiento, exponiendo al modelo a mayor variabilidad.

Las transformaciones aplicadas son:

\begin{enumerate}
    \item \textbf{Reflejo horizontal} (probabilidad 50\%): Simula variaciones en la orientación del paciente. Al reflejar la imagen, los puntos de referencia del lado izquierdo pasan a estar en el derecho y viceversa, por lo que se intercambian los pares simétricos (L3$\leftrightarrow$L4, L5$\leftrightarrow$L6, L7$\leftrightarrow$L8, L12$\leftrightarrow$L13, L14$\leftrightarrow$L15).

    \item \textbf{Rotación aleatoria} ($\pm$10 grados): Simula pequeñas variaciones en la posición del paciente durante la adquisición de la radiografía. Las coordenadas de los puntos de referencia se transforman aplicando la misma rotación para mantener la correspondencia.
\end{enumerate}

Estas transformaciones son \textit{conservadoras}: se limitan a variaciones que podrían ocurrir naturalmente en la adquisición de radiografías, evitando distorsiones que produzcan imágenes anatómicamente irrealistas.

\subsection{Resumen de Hiperparámetros}
\label{subsec:resumen_hiperparametros}

Los hiperparámetros son valores que definen la configuración del modelo y del proceso de entrenamiento, y que deben establecerse antes de iniciar el entrenamiento (a diferencia de los parámetros del modelo, que se aprenden automáticamente). La Tabla \ref{tab:hiperparametros_completos} consolida todos los hiperparámetros utilizados, organizados por categoría, para facilitar la reproducibilidad de los experimentos.

\begin{table}[htbp]
    \centering
    \caption{Resumen completo de hiperparámetros del modelo de puntos de referencia.}
    \label{tab:hiperparametros_completos}
    \begin{tabular}{llc}
        \toprule
        \textbf{Categoría} & \textbf{Parámetro} & \textbf{Valor} \\
        \midrule
        \multirow{5}{*}{Arquitectura}
            & Extractor de caracteristicas & ResNet-18 \\
            & Coordinate Attention & Habilitado (reduction=32) \\
            & Cabeza de regresión & 3 capas con GroupNorm \\
            & Dimensiones ocultas & 512 $\rightarrow$ 768 \\
            & Parámetros totales & $\sim$11.9M \\
        \midrule
        \multirow{3}{*}{Regularización}
            & Dropout (capa 1) & 0.3 \\
            & Dropout (capa 2) & 0.15 \\
            & Aumento de datos & Flip horizontal + Rotación $\pm$10° \\
        \midrule
        \multirow{3}{*}{Wing Loss}
            & $\omega$ & 10.0 px \\
            & $\epsilon$ & 2.0 px \\
            & Normalizado & Sí (escalado a [0,1]) \\
        \midrule
        \multirow{5}{*}{Fase 1}
            & Épocas & 15 \\
            & Tasa de aprendizaje & $1 \times 10^{-3}$ \\
            & Tamaño de lote & 16 \\
            & Extractor características & Congelado \\
            & Parada temprana & 5 épocas \\
        \midrule
        \multirow{6}{*}{Fase 2}
            & Épocas & 100 \\
            & LR extractor características + CA & $2 \times 10^{-5}$ \\
            & LR cabeza & $2 \times 10^{-4}$ \\
            & Tamaño de lote & 8 \\
            & Parada temprana & 15 épocas \\
            & Scheduler & Cosine Annealing \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Ensamble de Modelos}
\label{subsec:ensemble_modelos}

\paragraph{Motivación.}
Un modelo individual, por más bien entrenado que esté, puede cometer errores en ciertas imágenes debido a particularidades de su proceso de entrenamiento. Una estrategia para mitigar estos errores es consultar múltiples modelos y combinar sus predicciones, de manera análoga a solicitar la opinión de varios expertos antes de tomar una decisión. Esta técnica se denomina \textit{ensamble} (ensamble).

\paragraph{Por qué diferentes modelos producen diferentes predicciones.}
Aunque todos los modelos del ensamble utilizan la misma arquitectura y datos de entrenamiento, el proceso de entrenamiento involucra elementos aleatorios: el orden en que se presentan las imágenes, los valores iniciales de ciertos parámetros, y las muestras seleccionadas para cada lote. Estas diferencias, controladas por una \textit{semilla aleatoria}, hacen que cada modelo converja a una solución ligeramente diferente. Donde un modelo comete un error, otro puede acertar, y viceversa.

\subsubsection{Configuración del Ensemble}

Se entrenaron cuatro modelos utilizando la configuración descrita en las secciones anteriores, variando únicamente la semilla aleatoria ($123$, $321$, $111$, $666$). La predicción final se obtiene promediando las predicciones de los cuatro modelos:

\begin{equation}
    \hat{\mathbf{L}}_{\text{ensemble}} = \frac{1}{4} \sum_{k=1}^{4} \hat{\mathbf{L}}_k
    \label{eq:ensemble_average}
\end{equation}

\noindent donde $\hat{\mathbf{L}}_k$ representa las coordenadas predichas por el modelo $k$. Este promedio tiene el efecto de cancelar errores individuales: si un modelo predice un punto de referencia 2 píxeles a la izquierda de su posición real y otro lo predice 2 píxeles a la derecha, el promedio estará muy cerca de la posición correcta.

\subsubsection{Aumento en Tiempo de Prueba (TTA)}

Además del ensemble, se aplica una técnica complementaria llamada \textit{Aumento en Tiempo de Prueba} (TTA). Durante la inferencia (es decir, cuando el modelo ya está entrenado y se usa para predecir), cada imagen se procesa dos veces: en su orientación original y reflejada horizontalmente. Las predicciones de ambas versiones se promedian (intercambiando los puntos de referencia simétricos en la versión reflejada para que correspondan correctamente).

Esta técnica aprovecha la simetría anatómica del tórax: si el modelo predice correctamente un punto de referencia en la imagen original pero comete un pequeño error en la reflejada (o viceversa), el promedio reduce ese error. TTA no requiere entrenamiento adicional y se aplica únicamente durante la inferencia.
